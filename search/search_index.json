{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Quick-Tune-Tool docs.</p> <p>For a quick-start, check out examples for copy-pastable snippets to start from. </p>"},{"location":"examples/","title":"Examples","text":"<p>This houses the examples for the project. Use the navigation bar to the left to view more.</p>"},{"location":"examples/define_search_space/","title":"Define Search Space","text":"Expand to copy <code>examples/define_search_space.py</code>  (top right) <pre><code>from ConfigSpace import (\n    Categorical,\n    ConfigurationSpace,\n    EqualsCondition,\n    OrConjunction,\n    OrdinalHyperparameter,\n)\n\ncs = ConfigurationSpace(\"cv-classification\")\n\nfreeze = OrdinalHyperparameter(\"pct_to_freeze\", [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\nld = OrdinalHyperparameter(\"layer_decay\", [0.0, 0.65, 0.75])\nlp = OrdinalHyperparameter(\"linear_probing\", [False, True])\nsn = OrdinalHyperparameter(\"stoch_norm\", [False, True])\nsr = OrdinalHyperparameter(\"sp_reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\nd_reg = OrdinalHyperparameter(\"delta_reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\nbss = OrdinalHyperparameter(\"bss_reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\ncot = OrdinalHyperparameter(\"cotuning_reg\", [0.0, 0.5, 1.0, 2.0, 4.0])\n\nmix = OrdinalHyperparameter(\"mixup\", [0.0, 0.2, 0.4, 1.0, 2.0, 4.0, 8.0])\nmix_p = OrdinalHyperparameter(\"mixup_prob\", [0.0, 0.25, 0.5, 0.75, 1.0])\ncut = OrdinalHyperparameter(\"cutmix\", [0.0, 0.1, 0.25, 0.5, 1.0, 2.0, 4.0])\ndrop = OrdinalHyperparameter(\"drop\", [0.0, 0.1, 0.2, 0.3, 0.4])\nsmooth = OrdinalHyperparameter(\"smoothing\", [0.0, 0.05, 0.1])\nclip = OrdinalHyperparameter(\"clip_grad\", [0, 1, 10])\n\namp = OrdinalHyperparameter(\"amp\", [False, True])\nopt = Categorical(\"opt\", [\"sgd\", \"momentum\", \"adam\", \"adamw\", \"adamp\"])\nbetas = Categorical(\n    \"opt_betas\", [\"(0.9, 0.999)\", \"(0.0, 0.99)\", \"(0.9, 0.99)\", \"(0.0, 0.999)\"]\n)\nlr = OrdinalHyperparameter(\"lr\", [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01])\nw_ep = OrdinalHyperparameter(\"warmup_epochs\", [0, 5, 10])\nw_lr = OrdinalHyperparameter(\"warmup_lr\", [0.0, 1e-05, 1e-06])\nwd = OrdinalHyperparameter(\"weight_decay\", [0, 1e-05, 0.0001, 0.001, 0.01, 0.1])\nbs = OrdinalHyperparameter(\"batch_size\", [2, 4, 8, 16, 32, 64, 128, 256, 512])\nmom = OrdinalHyperparameter(\"momentum\", [0.0, 0.8, 0.9, 0.95, 0.99])\nsched = Categorical(\"sched\", [\"cosine\", \"step\", \"multistep\", \"plateau\"])\npe = OrdinalHyperparameter(\"patience_epochs\", [2, 5, 10])\ndr = OrdinalHyperparameter(\"decay_rate\", [0.1, 0.5])\nde = OrdinalHyperparameter(\"decay_epochs\", [10, 20])\nda = Categorical(\n    \"data_augmentation\",\n    [\"auto_augment\", \"random_augment\", \"trivial_augment\", \"none\"],\n)\naa = Categorical(\"auto_augment\", [\"v0\", \"original\"])\nra_nops = OrdinalHyperparameter(\"ra_num_ops\", [2, 3])\nra_mag = OrdinalHyperparameter(\"ra_magnitude\", [9, 17])\ncond_1 = EqualsCondition(pe, sched, \"plateau\")\ncond_2 = OrConjunction(\n    EqualsCondition(dr, sched, \"step\"),\n    EqualsCondition(dr, sched, \"multistep\"),\n)\ncond_3 = OrConjunction(\n    EqualsCondition(de, sched, \"step\"),\n    EqualsCondition(de, sched, \"multistep\"),\n)\ncond_4 = EqualsCondition(mom, opt, \"momentum\")\ncond_5 = OrConjunction(\n    EqualsCondition(betas, opt, \"adam\"),\n    EqualsCondition(betas, opt, \"adamw\"),\n    EqualsCondition(betas, opt, \"adamp\"),\n)\ncond_6 = EqualsCondition(ra_nops, da, \"random_augment\")\ncond_7 = EqualsCondition(ra_mag, da, \"random_augment\")\ncond_8 = EqualsCondition(aa, da, \"auto_augment\")\ncs.add(\n    mix,\n    mix_p,\n    cut,\n    drop,\n    smooth,\n    clip,\n    freeze,\n    ld,\n    lp,\n    sn,\n    sr,\n    d_reg,\n    bss,\n    cot,\n    amp,\n    opt,\n    betas,\n    lr,\n    w_ep,\n    w_lr,\n    wd,\n    bs,\n    mom,\n    sched,\n    pe,\n    dr,\n    de,\n    da,\n    aa,\n    ra_nops,\n    ra_mag,\n    cond_1,\n    cond_2,\n    cond_3,\n    cond_4,\n    cond_5,\n    cond_6,\n    cond_7,\n    cond_8,\n)\n\nmodel = Categorical(\n    \"model\",\n    [\n        \"beit_base_patch16_384\",\n        \"beit_large_patch16_512\",\n        \"convnext_small_384_in22ft1k\",\n        \"deit3_small_patch16_384_in21ft1k\",\n        \"dla46x_c\",\n        \"edgenext_small\",\n        \"edgenext_x_small\",\n        \"edgenext_xx_small\",\n        \"mobilevit_xs\",\n        \"mobilevit_xxs\",\n        \"mobilevitv2_075\",\n        \"swinv2_base_window12to24_192to384_22kft1k\",\n        \"tf_efficientnet_b4_ns\",\n        \"tf_efficientnet_b6_ns\",\n        \"tf_efficientnet_b7_ns\",\n        \"volo_d1_384\",\n        \"volo_d3_448\",\n        \"volo_d4_448\",\n        \"volo_d5_448\",\n        \"volo_d5_512\",\n        \"xcit_nano_12_p8_384_dist\",\n        \"xcit_small_12_p8_384_dist\",\n        \"xcit_tiny_12_p8_384_dist\",\n        \"xcit_tiny_24_p8_384_dist\",\n    ],\n)\ncs.add(model)\n</code></pre>"},{"location":"examples/define_search_space/#description","title":"Description","text":"<p>This examples shows how to define a search space. We use ConfigSpace.</p> <p>This search space is defined for a computer vision classification task and includes various hyperparameters that can be optimized.</p> <p>First import the necessary modules:</p> <pre><code>from ConfigSpace import (\n    Categorical,\n    ConfigurationSpace,\n    EqualsCondition,\n    OrConjunction,\n    OrdinalHyperparameter,\n)\n\ncs = ConfigurationSpace(\"cv-classification\")\n</code></pre>"},{"location":"examples/define_search_space/#finetuning-parameters","title":"Finetuning Parameters","text":"<p>The finetuning parameters in this configuration space are designed to control how a pre-trained model is fine-tuned on a new dataset. Here's a breakdown of each finetuning parameter:</p> <ol> <li> <p><code>pct_to_freeze</code> (Percentage of Model to Freeze): This parameter controls the fraction of the model's layers that will be frozen during training. Freezing a layer means that its weights will not be updated. Where <code>0.0</code> means no layers are frozen, and <code>1.0</code> means all layers are frozen, except for the final classification layer.</p> </li> <li> <p><code>layer_decay</code> (Layer-wise Learning Rate Decay): Layer-wise decay is a technique where deeper layers of the model use lower learning rates than layers closer to the output.</p> </li> <li> <p><code>linear_probing</code>: When linear probing is enabled, it means the training is focused on updating only the final classification layer (linear layer), while keeping the rest of the model frozen.</p> </li> <li> <p><code>stoch_norm</code> (Stochastic Normalization): Enabling stochastic normalization during training.</p> </li> <li> <p><code>sp_reg</code> (Starting Point Regularization): This parameter controls the amount of regularization applied to the weights of the model towards the pretrained model.</p> </li> <li> <p><code>delta_reg</code> (DELTA Regularization): DELTA regularization aims to preserve the outer layer outputs of the target network.</p> </li> <li> <p><code>bss_reg</code> (Batch Spectral Shrinkage Regularization): Batch Spectral Shrinkage (BSS) regularization penalizes the spectral norm of the model's weight matrices.</p> </li> <li> <p><code>cotuning_reg</code> (Co-tuning Regularization): This parameter controls the strength of co-tuning, a method that aligns the representation of new data with the pre-trained model's representations <pre><code>freeze = OrdinalHyperparameter(\"pct_to_freeze\", [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\nld = OrdinalHyperparameter(\"layer_decay\", [0.0, 0.65, 0.75])\nlp = OrdinalHyperparameter(\"linear_probing\", [False, True])\nsn = OrdinalHyperparameter(\"stoch_norm\", [False, True])\nsr = OrdinalHyperparameter(\"sp_reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\nd_reg = OrdinalHyperparameter(\"delta_reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\nbss = OrdinalHyperparameter(\"bss_reg\", [0.0, 0.0001, 0.001, 0.01, 0.1])\ncot = OrdinalHyperparameter(\"cotuning_reg\", [0.0, 0.5, 1.0, 2.0, 4.0])\n</code></pre></p> </li> </ol>"},{"location":"examples/define_search_space/#regularization-parameters","title":"Regularization Parameters","text":"<ul> <li> <p><code>mixup</code>: A data augmentation technique that mixes two training samples and their labels. The value determines the strength of mixing between samples.</p> </li> <li> <p><code>mixup_prob</code>: Specifies the probability of applying mixup augmentation to a given batch. A value of 0 means mixup is never applied, while 1 means it is applied to every batch.</p> </li> <li> <p><code>cutmix</code>: Another data augmentation method that combines portions of two images and their labels.</p> </li> <li> <p><code>drop</code> (Dropout): Dropout is a regularization technique where random neurons in a layer are \"dropped out\" (set to zero) during training.</p> </li> <li> <p><code>smoothing</code> (Label Smoothing): A technique that smooths the true labels, assigning a small probability to incorrect classes.</p> </li> <li> <p><code>clip_grad</code>: This controls the gradient clipping, which constrains the magnitude of gradients during backpropagation.</p> </li> </ul> <pre><code>mix = OrdinalHyperparameter(\"mixup\", [0.0, 0.2, 0.4, 1.0, 2.0, 4.0, 8.0])\nmix_p = OrdinalHyperparameter(\"mixup_prob\", [0.0, 0.25, 0.5, 0.75, 1.0])\ncut = OrdinalHyperparameter(\"cutmix\", [0.0, 0.1, 0.25, 0.5, 1.0, 2.0, 4.0])\ndrop = OrdinalHyperparameter(\"drop\", [0.0, 0.1, 0.2, 0.3, 0.4])\nsmooth = OrdinalHyperparameter(\"smoothing\", [0.0, 0.05, 0.1])\nclip = OrdinalHyperparameter(\"clip_grad\", [0, 1, 10])\n</code></pre>"},{"location":"examples/define_search_space/#optimization-parameters","title":"Optimization Parameters","text":"<pre><code>amp = OrdinalHyperparameter(\"amp\", [False, True])\nopt = Categorical(\"opt\", [\"sgd\", \"momentum\", \"adam\", \"adamw\", \"adamp\"])\nbetas = Categorical(\n    \"opt_betas\", [\"(0.9, 0.999)\", \"(0.0, 0.99)\", \"(0.9, 0.99)\", \"(0.0, 0.999)\"]\n)\nlr = OrdinalHyperparameter(\"lr\", [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005, 0.01])\nw_ep = OrdinalHyperparameter(\"warmup_epochs\", [0, 5, 10])\nw_lr = OrdinalHyperparameter(\"warmup_lr\", [0.0, 1e-05, 1e-06])\nwd = OrdinalHyperparameter(\"weight_decay\", [0, 1e-05, 0.0001, 0.001, 0.01, 0.1])\nbs = OrdinalHyperparameter(\"batch_size\", [2, 4, 8, 16, 32, 64, 128, 256, 512])\nmom = OrdinalHyperparameter(\"momentum\", [0.0, 0.8, 0.9, 0.95, 0.99])\nsched = Categorical(\"sched\", [\"cosine\", \"step\", \"multistep\", \"plateau\"])\npe = OrdinalHyperparameter(\"patience_epochs\", [2, 5, 10])\ndr = OrdinalHyperparameter(\"decay_rate\", [0.1, 0.5])\nde = OrdinalHyperparameter(\"decay_epochs\", [10, 20])\nda = Categorical(\n    \"data_augmentation\",\n    [\"auto_augment\", \"random_augment\", \"trivial_augment\", \"none\"],\n)\naa = Categorical(\"auto_augment\", [\"v0\", \"original\"])\nra_nops = OrdinalHyperparameter(\"ra_num_ops\", [2, 3])\nra_mag = OrdinalHyperparameter(\"ra_magnitude\", [9, 17])\ncond_1 = EqualsCondition(pe, sched, \"plateau\")\ncond_2 = OrConjunction(\n    EqualsCondition(dr, sched, \"step\"),\n    EqualsCondition(dr, sched, \"multistep\"),\n)\ncond_3 = OrConjunction(\n    EqualsCondition(de, sched, \"step\"),\n    EqualsCondition(de, sched, \"multistep\"),\n)\ncond_4 = EqualsCondition(mom, opt, \"momentum\")\ncond_5 = OrConjunction(\n    EqualsCondition(betas, opt, \"adam\"),\n    EqualsCondition(betas, opt, \"adamw\"),\n    EqualsCondition(betas, opt, \"adamp\"),\n)\ncond_6 = EqualsCondition(ra_nops, da, \"random_augment\")\ncond_7 = EqualsCondition(ra_mag, da, \"random_augment\")\ncond_8 = EqualsCondition(aa, da, \"auto_augment\")\ncs.add(\n    mix,\n    mix_p,\n    cut,\n    drop,\n    smooth,\n    clip,\n    freeze,\n    ld,\n    lp,\n    sn,\n    sr,\n    d_reg,\n    bss,\n    cot,\n    amp,\n    opt,\n    betas,\n    lr,\n    w_ep,\n    w_lr,\n    wd,\n    bs,\n    mom,\n    sched,\n    pe,\n    dr,\n    de,\n    da,\n    aa,\n    ra_nops,\n    ra_mag,\n    cond_1,\n    cond_2,\n    cond_3,\n    cond_4,\n    cond_5,\n    cond_6,\n    cond_7,\n    cond_8,\n)\n</code></pre>"},{"location":"examples/define_search_space/#model-choices","title":"Model Choices","text":"<p>The model choices represent a range of state-of-the-art deep learning architectures for image classification tasks. Each model has different characteristics in terms of architecture, size, and computational efficiency, providing flexibility to users depending on their specific needs and resources. Here's an overview:</p> <ul> <li> <p>Transformer-based models: These models, such as BEiT and DeiT, use the transformer architecture that has become popular in computer vision tasks. They are highly scalable and effective for large datasets and benefit from pre-training on extensive image corpora.</p> </li> <li> <p>ConvNet-based models: Models like ConvNeXt and EfficientNet are based on convolutional neural networks (CNNs), which have long been the standard for image classification.</p> </li> <li> <p>Lightweight models: Options such as MobileViT and EdgeNeXt are designed for resource-constrained environments like mobile devices or edge computing. These models prioritize smaller size and lower computational costs. <pre><code>model = Categorical(\n    \"model\",\n    [\n        \"beit_base_patch16_384\",\n        \"beit_large_patch16_512\",\n        \"convnext_small_384_in22ft1k\",\n        \"deit3_small_patch16_384_in21ft1k\",\n        \"dla46x_c\",\n        \"edgenext_small\",\n        \"edgenext_x_small\",\n        \"edgenext_xx_small\",\n        \"mobilevit_xs\",\n        \"mobilevit_xxs\",\n        \"mobilevitv2_075\",\n        \"swinv2_base_window12to24_192to384_22kft1k\",\n        \"tf_efficientnet_b4_ns\",\n        \"tf_efficientnet_b6_ns\",\n        \"tf_efficientnet_b7_ns\",\n        \"volo_d1_384\",\n        \"volo_d3_448\",\n        \"volo_d4_448\",\n        \"volo_d5_448\",\n        \"volo_d5_512\",\n        \"xcit_nano_12_p8_384_dist\",\n        \"xcit_small_12_p8_384_dist\",\n        \"xcit_tiny_12_p8_384_dist\",\n        \"xcit_tiny_24_p8_384_dist\",\n    ],\n)\ncs.add(model)\n</code></pre></p> </li> </ul>"},{"location":"examples/metatrain/","title":"Metatrain","text":"Expand to copy <code>examples/metatrain.py</code>  (top right) <pre><code>from qtt.predictors import PerfPredictor, CostPredictor\nimport pandas as pd\n\n\nconfig = pd.read_csv(\"config.csv\", index_col=0)  # pipeline configurations\nmeta = pd.read_csv(\"meta.csv\", index_col=0)  # if meta-features are available\ncurve = pd.read_csv(\"curve.csv\", index_col=0)  # learning curves\ncost = pd.read_csv(\"cost.csv\", index_col=0)  # runtime costs\n\nX = pd.concat([config, meta], axis=1)\ncurve = curve.values  # predictors expect curves as numpy arrays\ncost = cost.values  # predictors expect costs as numpy arrays\n\nperf_predictor = PerfPredictor().fit(X, curve)\ncost_predictor = CostPredictor().fit(X, cost)\n</code></pre>"},{"location":"examples/metatrain/#description","title":"Description","text":"<p><pre><code>from qtt.predictors import PerfPredictor, CostPredictor\nimport pandas as pd\n</code></pre> The <code>fit</code>-method of the predictors takes tabular data as input. If the data is stored in a CSV file, the expected format of the CSV is shown below:</p>"},{"location":"examples/metatrain/#configurations","title":"Configurations","text":"<p>Hyperparammeter configurations of previous evaluations. Do not apply any preprocessing to the data. Use native data types as much as possible.</p> model opt lr sched batch_size 1 xcit_abc adam 0.001 cosine 64 2 beit_def sgd 0.0005 step 128 3 mobilevit_xyz adamw 0.01 plateau 32 ..."},{"location":"examples/metatrain/#meta-features","title":"Meta-Features","text":"<p>Meta-features are optional. Meta-features refer to features that describe or summarize other features in a dataset. They are higher-level characteristics or properties of the dataset that can provide insight into its structure or complexity.</p> num-features num-classes 1 128 42 2 256 123 3 384 1000"},{"location":"examples/metatrain/#learning-curves","title":"Learning Curves","text":"<p>Learning curves show the performance of a model over time or over iterations as it learns from training data. For the vision classification task, the learning curves are the validation accuracy on the validation set.</p> 1 2 3 4 5 ... 1 0.11 0.12 0.13 0.14 0.15 ... 2 0.21 0.22 0.23 0.24 0.25 ... 3 0.31 0.32 0.33 0.34 0.35 ..."},{"location":"examples/metatrain/#cost","title":"Cost","text":"<p>The cost of running a pipeline (per fidelity). This refers to the total runtime required to complete the pipeline. This includes both the training and evaluation phases. We use the total runtime as the cost measure for each pipeline execution.</p> cost 1 12.3 2 45.6 3 78.9 <p>Ensure that the CSV files follow this structure for proper processing. <pre><code>config = pd.read_csv(\"config.csv\", index_col=0)  # pipeline configurations\nmeta = pd.read_csv(\"meta.csv\", index_col=0)  # if meta-features are available\ncurve = pd.read_csv(\"curve.csv\", index_col=0)  # learning curves\ncost = pd.read_csv(\"cost.csv\", index_col=0)  # runtime costs\n\nX = pd.concat([config, meta], axis=1)\ncurve = curve.values  # predictors expect curves as numpy arrays\ncost = cost.values  # predictors expect costs as numpy arrays\n\nperf_predictor = PerfPredictor().fit(X, curve)\ncost_predictor = CostPredictor().fit(X, cost)\n</code></pre></p>"},{"location":"examples/quicktuning/","title":"A quick example of using QuickCVCLSTuner to tune vision classifiers on a dataset.","text":"Expand to copy <code>examples/quicktuning.py</code>  (top right) <pre><code>\n</code></pre>"},{"location":"examples/quicktuning/#description","title":"Description","text":"<p>from qtt import QuickCVCLSTuner tuner = QuickCVCLSTuner(\"path/to/dataset\") tuner.run(fevals=100, time_budget=3600)</p>"},{"location":"examples/step_by_step/","title":"from qtt import QuickOptimizer, QuickTuner","text":"Expand to copy <code>examples/step_by_step.py</code>  (top right) <pre><code>\n</code></pre>"},{"location":"examples/step_by_step/#description","title":"Description","text":"<p>from qtt.predictors import PerfPredictor, CostPredictor from qtt.finetune.cv.classification import finetune_script, extract_task_info_metafeat import pandas as pd from ConfigSpace import ConfigurationSpace</p> <p>config = pd.read_csv(\"config.csv\", index_col=0)  # pipeline configurations meta = pd.read_csv(\"meta.csv\", index_col=0)  # if meta-features are available curve = pd.read_csv(\"curve.csv\", index_col=0)  # learning curves cost = pd.read_csv(\"cost.csv\", index_col=0)  # runtime costs</p> <p>X = pd.concat([config, meta], axis=1) curve = curve.values  # predictors expect curves as numpy arrays cost = cost.values  # predictors expect costs as numpy arrays</p> <p>perf_predictor = PerfPredictor().fit(X, curve) cost_predictor = CostPredictor().fit(X, cost)</p>"},{"location":"examples/step_by_step/#defineload-the-search-space","title":"Define/Load the search space","text":"<p>cs = ConfigurationSpace()  # ConfigurationSpace.from_json(\"cs.json\")</p>"},{"location":"examples/step_by_step/#define-the-optimizer","title":"Define the optimizer","text":"<p>optimizer = QuickOptimizer(     cs=cs,     max_fidelity=50,     perf_predictor=perf_predictor,     cost_predictor=cost_predictor, )</p> <p>task_info, metafeat = extract_task_info_metafeat(\"path/to/dataset\")</p> <p>optimizer.setup(     512,     metafeat=metafeat, )</p>"},{"location":"examples/step_by_step/#define-the-tuner","title":"Define the tuner","text":"<p>tuner = QuickTuner(     optimizer=optimizer,     f=finetune_script, ) tuner.run(task_info=task_info, fevals=100, time_budget=3600)</p>"},{"location":"reference/","title":"Index","text":""},{"location":"reference/optimizers/optimizer/","title":"Optimizer","text":""},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer","title":"<code>Optimizer</code>","text":"<p>Base class for all optimizers.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the subdirectory inside path where model will be saved. The final model directory will be os.path.join(path, name) If None, defaults to the model's class name: self.class.name</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>class Optimizer:\n    \"\"\"Base class for all optimizers.\n\n    Args:\n        path (str):\n            Directory location to store all outputs.\n            If None, a new unique time-stamped directory is chosen.\n        name (str):\n            Name of the subdirectory inside path where model will be saved.\n            The final model directory will be os.path.join(path, name)\n            If None, defaults to the model's class name: self.__class__.__name__\n    \"\"\"\n\n    model_file_name = \"model.pkl\"\n\n    def __init__(\n        self,\n        name: str | None = None,\n        path: str | None = None,\n    ):\n        if name is None:\n            self.name = self.__class__.__name__\n            logger.info(\n                f\"No name was specified for model, defaulting to class name: {self.name}\",\n            )\n        else:\n            self.name = name\n\n        if path is None:\n            self.path = setup_outputdir(path=self.name.lower())\n            logger.info(\n                f\"No path was specified for predictor, defaulting to: {self.path}\",\n            )\n        else:\n            self.path = setup_outputdir(path)\n\n        self._is_initialized = False\n\n    def ante(self):\n        \"\"\"This method is intended for the use with a tuner.\n        It allows to perform some pre-processing steps before each ask.\"\"\"\n        pass\n\n    def post(self):\n        \"\"\"This method is intended for the use with a tuner.\n        It allows to perform some post-processing steps after each tell.\"\"\"\n        pass\n\n    def ask(self) -&gt; dict:\n        \"\"\"Ask the optimizer for a trial to evaluate.\n\n        Returns:\n            A config to sample.\n        \"\"\"\n        raise NotImplementedError\n\n    def tell(self, report: dict | list[dict]):\n        \"\"\"Tell the optimizer the result for an asked trial.\n\n        Args:\n            report (dict): The result for a trial\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n        \"\"\"\n        Loads the model from disk to memory.\n\n        Args:\n            path (str):\n                Path to the saved model, minus the file name.\n                This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n                The model file is typically located in os.path.join(path, cls.model_file_name).\n            reset_paths (bool):\n                Whether to reset the self.path value of the loaded model to be equal to path.\n                It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n                If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n            verbose (bool):\n                Whether to log the location of the loaded file.\n\n        Returns:\n            model (Optimizer):\n                Loaded model object.\n        \"\"\"\n        file_path = os.path.join(path, cls.model_file_name)\n        with open(file_path, \"rb\") as f:\n            model = pickle.load(f)\n        if reset_paths:\n            model.path = path\n        if verbose:\n            logger.info(f\"Model loaded from: {file_path}\")\n        return model\n\n    def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n        \"\"\"\n        Saves the model to disk.\n\n        Args:\n            path (str):\n                Path to the saved model, minus the file name.\n                This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n                If None, self.path is used.\n                The final model file is typically saved to os.path.join(path, self.model_file_name).\n            verbose (bool):\n                Whether to log the location of the saved file.\n\n        Returns:\n            path (str):\n                Path to the saved model, minus the file name.\n                Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as model = RFModel.load(path)\n        \"\"\"\n        if path is None:\n            path = self.path\n        os.makedirs(path, exist_ok=True)\n        file_path = os.path.join(path, self.model_file_name)\n        # tmp = {}\n        # for key, obj in vars(self).items():\n        #     if hasattr(obj, \"save\"):\n        #         obj_path = os.path.join(path, key)\n        #         obj.save(obj_path)\n        #         tmp[key] = obj\n        #         setattr(self, key, None)\n        with open(file_path, \"wb\") as f:\n            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n        # for key, obj in tmp.items():\n        #     setattr(self, key, obj)\n        if verbose:\n            logger.info(f\"Model saved to: {file_path}\")\n        return path\n\n    def reset_path(self, path: str | None = None):\n        \"\"\"Reset the path of the model.\n\n        Args:\n            path (str):\n                Directory location to store all outputs.\n                If None, a new unique time-stamped directory is chosen.\n        \"\"\"\n        if path is None:\n            path = setup_outputdir(path=self.name.lower(), path_suffix=self.name)\n        self.path = path\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.ante","title":"<code>ante()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some pre-processing steps before each ask.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def ante(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some pre-processing steps before each ask.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.ask","title":"<code>ask()</code>","text":"<p>Ask the optimizer for a trial to evaluate.</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>A config to sample.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def ask(self) -&gt; dict:\n    \"\"\"Ask the optimizer for a trial to evaluate.\n\n    Returns:\n        A config to sample.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). The model file is typically located in os.path.join(path, cls.model_file_name).</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the self.path value of the loaded model to be equal to path. It is highly recommended to keep this value as True unless accessing the original self.path value is important. If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Optimizer</code> )          \u2013            <pre><code>Loaded model object.\n</code></pre> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n    \"\"\"\n    Loads the model from disk to memory.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths (bool):\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose (bool):\n            Whether to log the location of the loaded file.\n\n    Returns:\n        model (Optimizer):\n            Loaded model object.\n    \"\"\"\n    file_path = os.path.join(path, cls.model_file_name)\n    with open(file_path, \"rb\") as f:\n        model = pickle.load(f)\n    if reset_paths:\n        model.path = path\n    if verbose:\n        logger.info(f\"Model loaded from: {file_path}\")\n    return model\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.post","title":"<code>post()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some post-processing steps after each tell.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def post(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some post-processing steps after each tell.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"Reset the path of the model.\n\n    Args:\n        path (str):\n            Directory location to store all outputs.\n            If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower(), path_suffix=self.name)\n    self.path = path\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.save","title":"<code>save(path=None, verbose=True)</code>","text":"<p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). If None, self.path is used. The final model file is typically saved to os.path.join(path, self.model_file_name).</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the saved file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>path</code> (              <code>str</code> )          \u2013            <pre><code>Path to the saved model, minus the file name.\nUse this value to load the model from disk via cls.load(path), cls being the class of the model object, such as model = RFModel.load(path)\n</code></pre> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Saves the model to disk.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            If None, self.path is used.\n            The final model file is typically saved to os.path.join(path, self.model_file_name).\n        verbose (bool):\n            Whether to log the location of the saved file.\n\n    Returns:\n        path (str):\n            Path to the saved model, minus the file name.\n            Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as model = RFModel.load(path)\n    \"\"\"\n    if path is None:\n        path = self.path\n    os.makedirs(path, exist_ok=True)\n    file_path = os.path.join(path, self.model_file_name)\n    # tmp = {}\n    # for key, obj in vars(self).items():\n    #     if hasattr(obj, \"save\"):\n    #         obj_path = os.path.join(path, key)\n    #         obj.save(obj_path)\n    #         tmp[key] = obj\n    #         setattr(self, key, None)\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n    # for key, obj in tmp.items():\n    #     setattr(self, key, obj)\n    if verbose:\n        logger.info(f\"Model saved to: {file_path}\")\n    return path\n</code></pre>"},{"location":"reference/optimizers/optimizer/#qtt.optimizers.optimizer.Optimizer.tell","title":"<code>tell(report)</code>","text":"<p>Tell the optimizer the result for an asked trial.</p> <p>Parameters:</p> <ul> <li> <code>report</code>               (<code>dict</code>)           \u2013            <p>The result for a trial</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def tell(self, report: dict | list[dict]):\n    \"\"\"Tell the optimizer the result for an asked trial.\n\n    Args:\n        report (dict): The result for a trial\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/optimizers/quick/","title":"Quick","text":""},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer","title":"<code>QuickOptimizer</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>QuickOptimizer implements a cost-aware Bayesian optimization approach with an ask and tell interface. It uses a DyHPO predictor for performance and a simple MLP for cost.</p> <p>Parameters:</p> <ul> <li> <code>cs</code>               (<code>ConfigurationSpace</code>)           \u2013            <p>The configuration space to optimize over.</p> </li> <li> <code>max_fidelity</code>               (<code>int</code>)           \u2013            <p>The maximum fidelity to optimize. Fidelity is a measure of a resource used by a configuration, such as the number of epochs.</p> </li> <li> <code>perf_predictor</code>               (<code>PerfPredictor</code>, default:                   <code>None</code> )           \u2013            <p>The performance predictor to use. If None, a new predictor is created.</p> </li> <li> <code>cost_predictor</code>               (<code>CostPredictor</code>, default:                   <code>None</code> )           \u2013            <p>The cost predictor to use. If None, a new CostPredictor is created if <code>cost_aware</code> is True.</p> </li> <li> <code>cost_aware</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use the cost predictor. Defaults to False.</p> </li> <li> <code>cost_factor</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>A factor to control the scaling of cost values. Values must be in the range <code>[0.0, inf)</code>. A cost factor smaller than 1 compresses the cost values closer together (with 0 equalizing them), while values larger than 1 expand them. Defaults to 1.0.</p> </li> <li> <code>acq_fn</code>               (<code>str</code>, default:                   <code>'ei'</code> )           \u2013            <p>The acquisition function to use. One of [\"ei\", \"ucb\", \"thompson\", \"exploit\"]. Defaults to \"ei\".</p> </li> <li> <code>explore_factor</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The exploration factor in the acquisition function. Defaults to 1.0.</p> </li> <li> <code>patience</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Determines if early stopping should be applied for a single configuration. If the score does not improve for <code>patience</code> steps, the configuration is stopped. Defaults to None.</p> </li> <li> <code>tol</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Tolerance for early stopping. Training stops if the score does not improve by at least <code>tol</code> for <code>patience</code> iterations (if set). Values must be in the range <code>[0.0, inf)</code>. Defaults to 0.0.</p> </li> <li> <code>score_thresh</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Threshold for early stopping. If the score is above <code>1 - score_thresh</code>, the configuration is stopped. Defaults to 0.0.</p> </li> <li> <code>init_random_search_steps</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of configurations to evaluate randomly at the beginning of the optimization (with fidelity 1) before using predictors/acquisition function. Defaults to 10.</p> </li> <li> <code>refit_init_steps</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of steps (successful evaluations) before refitting the predictors. Defaults to 0.</p> </li> <li> <code>refit</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to refit the predictors with observed data. Defaults to False.</p> </li> <li> <code>refit_interval</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Interval for refitting the predictors. Defaults to 1.</p> </li> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to save the optimizer state. Defaults to None.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Seed for reproducibility. Defaults to None.</p> </li> <li> <code>verbosity</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity level for logging. Defaults to 2.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>class QuickOptimizer(Optimizer):\n    \"\"\"QuickOptimizer implements a cost-aware Bayesian optimization approach with an ask\n    and tell interface. It uses a DyHPO predictor for performance and a simple MLP for\n    cost.\n\n    Args:\n        cs (ConfigurationSpace): The configuration space to optimize over.\n        max_fidelity (int): The maximum fidelity to optimize. Fidelity is a measure of\n            a resource used by a configuration, such as the number of epochs.\n        perf_predictor (PerfPredictor, optional): The performance predictor to use. If\n            None, a new predictor is created.\n        cost_predictor (CostPredictor, optional): The cost predictor to use. If None,\n            a new CostPredictor is created if `cost_aware` is True.\n        cost_aware (bool, optional): Whether to use the cost predictor. Defaults to False.\n        cost_factor (float, optional): A factor to control the scaling of cost values.\n            Values must be in the range `[0.0, inf)`. A cost factor smaller than 1\n            compresses the cost values closer together (with 0 equalizing them), while\n            values larger than 1 expand them. Defaults to 1.0.\n        acq_fn (str, optional): The acquisition function to use. One of [\"ei\", \"ucb\",\n            \"thompson\", \"exploit\"]. Defaults to \"ei\".\n        explore_factor (float, optional): The exploration factor in the acquisition\n            function. Defaults to 1.0.\n        patience (int, optional): Determines if early stopping should be applied for a\n            single configuration. If the score does not improve for `patience` steps,\n            the configuration is stopped. Defaults to None.\n        tol (float, optional): Tolerance for early stopping. Training stops if the score\n            does not improve by at least `tol` for `patience` iterations (if set). Values\n            must be in the range `[0.0, inf)`. Defaults to 0.0.\n        score_thresh (float, optional): Threshold for early stopping. If the score is\n            above `1 - score_thresh`, the configuration is stopped. Defaults to 0.0.\n        init_random_search_steps (int, optional): Number of configurations to evaluate\n            randomly at the beginning of the optimization (with fidelity 1) before using\n            predictors/acquisition function. Defaults to 10.\n        refit_init_steps (int, optional): Number of steps (successful evaluations) before\n            refitting the predictors. Defaults to 0.\n        refit (bool, optional): Whether to refit the predictors with observed data.\n            Defaults to False.\n        refit_interval (int, optional): Interval for refitting the predictors. Defaults\n            to 1.\n        path (str, optional): Path to save the optimizer state. Defaults to None.\n        seed (int, optional): Seed for reproducibility. Defaults to None.\n        verbosity (int, optional): Verbosity level for logging. Defaults to 2.\n    \"\"\"\n\n    def __init__(\n        self,\n        cs: ConfigurationSpace,\n        max_fidelity: int,\n        perf_predictor: PerfPredictor | None = None,\n        cost_predictor: CostPredictor | None = None,\n        *,\n        cost_aware: bool = False,\n        cost_factor: float = 1.0,\n        acq_fn: Literal[\"ei\", \"ucb\", \"thompson\", \"exploit\"] = \"ei\",\n        explore_factor: float = 0.0,\n        patience: int | None = None,\n        tol: float = 1e-4,\n        score_thresh: float = 0.0,\n        init_random_search_steps: int = 3,\n        refit_init_steps: int = 0,\n        refit: bool = False,\n        refit_interval: int = 1,\n        #\n        path: str | None = None,\n        seed: int | None = None,\n        verbosity: int = 2,\n    ):\n        super().__init__(path=path)\n        set_logger_verbosity(verbosity, logger)\n        self.verbosity = verbosity\n\n        if seed is not None:\n            fix_random_seeds(seed)\n        self.seed = seed\n\n        # configuration space\n        self.cs = cs\n        self.max_fidelity = max_fidelity\n\n        # optimizer related parameters\n        self.acq_fn = acq_fn\n        self.explore_factor = explore_factor\n        self.cost_aware = cost_aware\n        self.cost_factor = cost_factor\n        self.patience = patience\n        self.tol = tol\n        self.scr_thr = score_thresh\n        self.refit_init_steps = refit_init_steps\n        self.refit = refit\n        self.refit_interval = refit_interval\n\n        # predictors\n        self.perf_predictor = perf_predictor\n        if self.perf_predictor is None:\n            self.perf_predictor = PerfPredictor(path=path)\n        self.cost_predictor = cost_predictor\n        if self.cost_aware and self.cost_predictor is None:\n            self.cost_predictor = CostPredictor(path=path)\n\n        # trackers\n        self.init_random_search_steps = init_random_search_steps\n        self.ask_count = 0\n        self.tell_count = 0\n        self.init_count = 0\n        self.eval_count = 0\n        self.configs: list[dict] = []\n        self.evaled = set()\n        self.stoped = set()\n        self.failed = set()\n        self.history = []\n\n        # placeholders\n        self.pipelines: pd.DataFrame\n        self.curves: np.ndarray\n        self.fidelities: np.ndarray\n        self.costs: np.ndarray | None = None\n        self.score_history: np.ndarray | None = None\n\n        # flags\n        self.ready = False\n        self.finished = False\n\n    def setup(\n        self,\n        n: int,\n        metafeat: Mapping[str, int | float] | None = None,\n    ) -&gt; None:\n        \"\"\"Setup the optimizer for optimization.\n\n        Create the configurations to evaluate. The configurations are sampled from the\n        configuration space. Optionally, metafeatures of the dataset can be provided.\n\n        Args:\n            n (int): The number of configurations to create.\n            metafeat (Mapping[str, int | float], optional): The metafeatures of the dataset.\n        \"\"\"\n        self.N = n\n        self.fidelities: np.ndarray = np.zeros(n, dtype=int)\n        self.curves: np.ndarray = np.full((n, self.max_fidelity), np.nan, dtype=float)\n        self.costs = None\n        if self.patience is not None:\n            self.score_history = np.zeros((n, self.patience), dtype=float)\n\n        if self.seed is not None:\n            self.cs.seed(self.seed)\n        _configs = self.cs.sample_configuration(n)\n        self.configs = [dict(c) for c in _configs]\n        self.pipelines = pd.DataFrame(self.configs)\n\n        self.metafeat = metafeat\n        if self.metafeat is not None:\n            self.metafeat = pd.DataFrame([metafeat] * self.N)\n        self.pipelines = pd.concat([self.pipelines, self.metafeat], axis=1)\n\n        self.ready = True\n\n    def setup_pandas(\n        self,\n        df: pd.DataFrame,\n        metafeat: Mapping[str, int | float] | None = None,\n    ):\n        \"\"\"Setup the optimizer for optimization.\n\n        Use an existing DataFrame to create the configurations to evaluate. Optionally,\n        metafeatures of the dataset can be provided.\n\n        Args:\n            df (pd.DataFrame): The DataFrame with the configurations to evaluate.\n            metafeat (Mapping[str, int | float], optional): The metafeatures of the dataset.\n        \"\"\"\n        self.pipelines = df\n        self.N = len(df)\n        self.fidelities: np.ndarray = np.zeros(self.N, dtype=int)\n        self.curves: np.ndarray = np.full(\n            (self.N, self.max_fidelity), np.nan, dtype=float\n        )\n        self.costs = None\n        if self.patience is not None:\n            self.score_history = np.zeros((self.N, self.patience), dtype=float)\n\n        self.metafeat = metafeat\n        if self.metafeat is not None:\n            self.metafeat = pd.DataFrame([metafeat] * self.N)\n        self.pipelines = pd.concat([self.pipelines, self.metafeat], axis=1)\n        self.configs = self.pipelines.to_dict(orient=\"records\")\n\n        self.ready = True\n\n    def _predict(self) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray | None]:\n        \"\"\"Predict the performance and cost of the configurations.\n\n        Returns:\n            The mean and standard deviation of the performance of the pipelines and their costs.\n        \"\"\"\n        pipeline, curve = self.pipelines, self.curves\n\n        pred = self.perf_predictor.predict(pipeline, curve)  # type: ignore\n        pred_mean, pred_std = pred\n\n        costs = self.costs\n        if self.cost_aware and self.costs is None:\n            costs = self.cost_predictor.predict(pipeline)  # type: ignore\n            costs = np.clip(costs, 1e-6, None)  # avoid division by zero\n            costs /= costs.max()  # normalize\n            costs = np.power(costs, self.cost_factor)  # rescale\n            self.costs = costs\n\n        return pred_mean, pred_std, costs\n\n    def _calc_acq_val(self, mean, std, y_max):\n        \"\"\"Calculate the acquisition value.\n\n        Args:\n            mean: np.ndarray\n                The mean of the predictions.\n            std: np.ndarray\n                The standard deviation of the predictions.\n            y_max: np.ndarray\n                The maximum score per fidelity.\n\n        Returns:\n            The acquisition values.\n        \"\"\"\n        fn = self.acq_fn\n        xi = self.explore_factor\n        match fn:\n            # Expected Improvement\n            case \"ei\":\n                mask = std == 0\n                std = std + mask * 1.0\n                z = (mean - y_max - xi) / std\n                acq_value = (mean - y_max) * norm.cdf(z) + std * norm.pdf(z)\n                acq_value[mask] = 0.0\n            # Upper Confidence Bound\n            case \"ucb\":\n                acq_value = mean + xi * std\n            # Thompson Sampling\n            case \"thompson\":\n                acq_value = np.random.normal(mean, std)\n            # Exploitation\n            case \"exploit\":\n                acq_value = mean\n            case _:\n                raise ValueError\n        return acq_value\n\n    def _optimize_acq_fn(self, mean, std, cost) -&gt; list[int]:\n        \"\"\"Optimize the acquisition function.\n\n        Args:\n            mean: np.ndarray\n                The mean of the predictions.\n            std: np.ndarray\n                The standard deviation of the predictions.\n            cost: np.ndarray\n                The cost of the pipeline.\n\n        Returns:\n            A sorted list of indices of the pipeline.\n        \"\"\"\n        # maximum score per fidelity\n        curves = np.nan_to_num(self.curves)\n        y_max = curves.max(axis=0)\n        y_max = np.maximum.accumulate(y_max)\n\n        # get the ymax for the next fidelity of the pipelines\n        next_fidelitys = np.minimum(self.fidelities + 1, self.max_fidelity)\n        y_max_next = y_max[next_fidelitys - 1]\n\n        acq_values = self._calc_acq_val(mean, std, y_max_next)\n        if self.cost_aware:\n            acq_values /= cost\n\n        return np.argsort(acq_values).tolist()\n\n    def _ask(self):\n        pred_mean, pred_std, cost = self._predict()\n        ranks = self._optimize_acq_fn(pred_mean, pred_std, cost)\n        ranks = [r for r in ranks if r not in self.stoped | self.failed]\n        index = ranks[-1]\n        logger.debug(f\"predicted score: {pred_mean[index]:.4f}\")\n        return index\n\n    def ask(self) -&gt; dict | None:\n        \"\"\"Ask the optimizer for a configuration to evaluate.\n\n        Returns:\n            A dictionary with the configuration to evaluate.\n        \"\"\"\n        if not self.ready:\n            raise RuntimeError(\"Call setup() before ask()\")\n\n        if self.finished:\n            return None\n\n        self.ask_count += 1\n        if len(self.evaled) &lt; self.init_random_search_steps:\n            left = set(range(self.N)) - self.evaled - self.failed - self.stoped\n            index = left.pop()\n            fidelity = 1\n        else:\n            index = self._ask()\n            fidelity = self.fidelities[index] + 1\n\n        return {\n            \"config_id\": index,\n            \"config\": self.configs[index],\n            \"fidelity\": fidelity,\n        }\n\n    def tell(self, result: dict | list):\n        \"\"\"Tell the result of a trial to the optimizer.\n\n        Args:\n            result: dict | list[dict]\n                The result(s) for a trial.\n        \"\"\"\n        if isinstance(result, dict):\n            result = [result]\n        for res in result:\n            self._tell(res)\n\n    def _tell(self, result: dict):\n        self.tell_count += 1\n\n        index = result[\"config_id\"]\n        fidelity = result[\"fidelity\"]\n        # cost = result[\"cost\"]\n        score = result[\"score\"]\n        status = result[\"status\"]\n\n        if not status:\n            self.failed.add(index)\n            return\n\n        if score &gt;= 1.0 - self.scr_thr or fidelity == self.max_fidelity:\n            self.stoped.add(index)\n\n        # update trackers\n        self.curves[index, fidelity - 1] = score\n        self.fidelities[index] = fidelity\n        # self.costs[index] = cost\n        self.history.append(result)\n        self.evaled.add(index)\n        self.eval_count += 1\n\n        if self.patience is not None:\n            assert self.score_history is not None\n            if not np.any(self.score_history[index] &lt; (score - self.tol)):\n                self.stoped.add(index)\n            self.score_history[index][fidelity % self.patience] = score\n\n        self.finished = self._check_is_finished()\n\n    def _check_is_finished(self):\n        \"\"\"Check if there is no more configurations to evaluate.\"\"\"\n        left = set(range(self.N)) - self.evaled - self.failed - self.stoped\n        if not left:\n            return True\n        return False\n\n    def ante(self):\n        \"\"\"Some operations to perform by the tuner before the optimization loop\n\n        Here: refit the predictors with observed data.\n        \"\"\"\n        if (\n            self.refit\n            and not self.eval_count % self.refit_interval\n            and self.eval_count &gt;= self.refit_init_steps\n        ):\n            self.fit_extra()\n\n    def fit_extra(self):\n        \"\"\"Refit the predictors with observed data.\"\"\"\n        pipeline, curve = self.pipelines, self.curves\n        self.perf_predictor.fit_extra(pipeline, curve)  # type: ignore\n\n    def fit(self, X, curve, cost):\n        \"\"\"\n        Fit the predictors with the given training data.\n        \"\"\"\n        self.perf_predictor.fit(X, curve)  # type: ignore\n        if self.cost_predictor is not None:\n            self.cost_predictor.fit(X, cost)\n\n    def reset_path(self, path: str | None = None):\n        \"\"\"\n        Reset the path of the model.\n        Parameters\n        ----------\n        path : str, default = None\n            Directory location to store all outputs.\n            If None, a new unique time-stamped directory is chosen.\n        \"\"\"\n        super().reset_path(path)\n        if self.perf_predictor is not None:\n            self.perf_predictor.reset_path(path)\n        if self.cost_predictor is not None:\n            self.cost_predictor.reset_path(path)\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.ante","title":"<code>ante()</code>","text":"<p>Some operations to perform by the tuner before the optimization loop</p> <p>Here: refit the predictors with observed data.</p> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def ante(self):\n    \"\"\"Some operations to perform by the tuner before the optimization loop\n\n    Here: refit the predictors with observed data.\n    \"\"\"\n    if (\n        self.refit\n        and not self.eval_count % self.refit_interval\n        and self.eval_count &gt;= self.refit_init_steps\n    ):\n        self.fit_extra()\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.ask","title":"<code>ask()</code>","text":"<p>Ask the optimizer for a configuration to evaluate.</p> <p>Returns:</p> <ul> <li> <code>dict | None</code>           \u2013            <p>A dictionary with the configuration to evaluate.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def ask(self) -&gt; dict | None:\n    \"\"\"Ask the optimizer for a configuration to evaluate.\n\n    Returns:\n        A dictionary with the configuration to evaluate.\n    \"\"\"\n    if not self.ready:\n        raise RuntimeError(\"Call setup() before ask()\")\n\n    if self.finished:\n        return None\n\n    self.ask_count += 1\n    if len(self.evaled) &lt; self.init_random_search_steps:\n        left = set(range(self.N)) - self.evaled - self.failed - self.stoped\n        index = left.pop()\n        fidelity = 1\n    else:\n        index = self._ask()\n        fidelity = self.fidelities[index] + 1\n\n    return {\n        \"config_id\": index,\n        \"config\": self.configs[index],\n        \"fidelity\": fidelity,\n    }\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.fit","title":"<code>fit(X, curve, cost)</code>","text":"<p>Fit the predictors with the given training data.</p> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def fit(self, X, curve, cost):\n    \"\"\"\n    Fit the predictors with the given training data.\n    \"\"\"\n    self.perf_predictor.fit(X, curve)  # type: ignore\n    if self.cost_predictor is not None:\n        self.cost_predictor.fit(X, cost)\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.fit_extra","title":"<code>fit_extra()</code>","text":"<p>Refit the predictors with observed data.</p> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def fit_extra(self):\n    \"\"\"Refit the predictors with observed data.\"\"\"\n    pipeline, curve = self.pipelines, self.curves\n    self.perf_predictor.fit_extra(pipeline, curve)  # type: ignore\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). The model file is typically located in os.path.join(path, cls.model_file_name).</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the self.path value of the loaded model to be equal to path. It is highly recommended to keep this value as True unless accessing the original self.path value is important. If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Optimizer</code> )          \u2013            <pre><code>Loaded model object.\n</code></pre> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n    \"\"\"\n    Loads the model from disk to memory.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths (bool):\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose (bool):\n            Whether to log the location of the loaded file.\n\n    Returns:\n        model (Optimizer):\n            Loaded model object.\n    \"\"\"\n    file_path = os.path.join(path, cls.model_file_name)\n    with open(file_path, \"rb\") as f:\n        model = pickle.load(f)\n    if reset_paths:\n        model.path = path\n    if verbose:\n        logger.info(f\"Model loaded from: {file_path}\")\n    return model\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.post","title":"<code>post()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some post-processing steps after each tell.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def post(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some post-processing steps after each tell.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model. Parameters</p> <p>path : str, default = None     Directory location to store all outputs.     If None, a new unique time-stamped directory is chosen.</p> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"\n    Reset the path of the model.\n    Parameters\n    ----------\n    path : str, default = None\n        Directory location to store all outputs.\n        If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    super().reset_path(path)\n    if self.perf_predictor is not None:\n        self.perf_predictor.reset_path(path)\n    if self.cost_predictor is not None:\n        self.cost_predictor.reset_path(path)\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.save","title":"<code>save(path=None, verbose=True)</code>","text":"<p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). If None, self.path is used. The final model file is typically saved to os.path.join(path, self.model_file_name).</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the saved file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>path</code> (              <code>str</code> )          \u2013            <pre><code>Path to the saved model, minus the file name.\nUse this value to load the model from disk via cls.load(path), cls being the class of the model object, such as model = RFModel.load(path)\n</code></pre> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Saves the model to disk.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            If None, self.path is used.\n            The final model file is typically saved to os.path.join(path, self.model_file_name).\n        verbose (bool):\n            Whether to log the location of the saved file.\n\n    Returns:\n        path (str):\n            Path to the saved model, minus the file name.\n            Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as model = RFModel.load(path)\n    \"\"\"\n    if path is None:\n        path = self.path\n    os.makedirs(path, exist_ok=True)\n    file_path = os.path.join(path, self.model_file_name)\n    # tmp = {}\n    # for key, obj in vars(self).items():\n    #     if hasattr(obj, \"save\"):\n    #         obj_path = os.path.join(path, key)\n    #         obj.save(obj_path)\n    #         tmp[key] = obj\n    #         setattr(self, key, None)\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n    # for key, obj in tmp.items():\n    #     setattr(self, key, obj)\n    if verbose:\n        logger.info(f\"Model saved to: {file_path}\")\n    return path\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.setup","title":"<code>setup(n, metafeat=None)</code>","text":"<p>Setup the optimizer for optimization.</p> <p>Create the configurations to evaluate. The configurations are sampled from the configuration space. Optionally, metafeatures of the dataset can be provided.</p> <p>Parameters:</p> <ul> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>The number of configurations to create.</p> </li> <li> <code>metafeat</code>               (<code>Mapping[str, int | float]</code>, default:                   <code>None</code> )           \u2013            <p>The metafeatures of the dataset.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def setup(\n    self,\n    n: int,\n    metafeat: Mapping[str, int | float] | None = None,\n) -&gt; None:\n    \"\"\"Setup the optimizer for optimization.\n\n    Create the configurations to evaluate. The configurations are sampled from the\n    configuration space. Optionally, metafeatures of the dataset can be provided.\n\n    Args:\n        n (int): The number of configurations to create.\n        metafeat (Mapping[str, int | float], optional): The metafeatures of the dataset.\n    \"\"\"\n    self.N = n\n    self.fidelities: np.ndarray = np.zeros(n, dtype=int)\n    self.curves: np.ndarray = np.full((n, self.max_fidelity), np.nan, dtype=float)\n    self.costs = None\n    if self.patience is not None:\n        self.score_history = np.zeros((n, self.patience), dtype=float)\n\n    if self.seed is not None:\n        self.cs.seed(self.seed)\n    _configs = self.cs.sample_configuration(n)\n    self.configs = [dict(c) for c in _configs]\n    self.pipelines = pd.DataFrame(self.configs)\n\n    self.metafeat = metafeat\n    if self.metafeat is not None:\n        self.metafeat = pd.DataFrame([metafeat] * self.N)\n    self.pipelines = pd.concat([self.pipelines, self.metafeat], axis=1)\n\n    self.ready = True\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.setup_pandas","title":"<code>setup_pandas(df, metafeat=None)</code>","text":"<p>Setup the optimizer for optimization.</p> <p>Use an existing DataFrame to create the configurations to evaluate. Optionally, metafeatures of the dataset can be provided.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>The DataFrame with the configurations to evaluate.</p> </li> <li> <code>metafeat</code>               (<code>Mapping[str, int | float]</code>, default:                   <code>None</code> )           \u2013            <p>The metafeatures of the dataset.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def setup_pandas(\n    self,\n    df: pd.DataFrame,\n    metafeat: Mapping[str, int | float] | None = None,\n):\n    \"\"\"Setup the optimizer for optimization.\n\n    Use an existing DataFrame to create the configurations to evaluate. Optionally,\n    metafeatures of the dataset can be provided.\n\n    Args:\n        df (pd.DataFrame): The DataFrame with the configurations to evaluate.\n        metafeat (Mapping[str, int | float], optional): The metafeatures of the dataset.\n    \"\"\"\n    self.pipelines = df\n    self.N = len(df)\n    self.fidelities: np.ndarray = np.zeros(self.N, dtype=int)\n    self.curves: np.ndarray = np.full(\n        (self.N, self.max_fidelity), np.nan, dtype=float\n    )\n    self.costs = None\n    if self.patience is not None:\n        self.score_history = np.zeros((self.N, self.patience), dtype=float)\n\n    self.metafeat = metafeat\n    if self.metafeat is not None:\n        self.metafeat = pd.DataFrame([metafeat] * self.N)\n    self.pipelines = pd.concat([self.pipelines, self.metafeat], axis=1)\n    self.configs = self.pipelines.to_dict(orient=\"records\")\n\n    self.ready = True\n</code></pre>"},{"location":"reference/optimizers/quick/#qtt.optimizers.quick.QuickOptimizer.tell","title":"<code>tell(result)</code>","text":"<p>Tell the result of a trial to the optimizer.</p> <p>Parameters:</p> <ul> <li> <code>result</code>               (<code>dict | list</code>)           \u2013            <p>dict | list[dict] The result(s) for a trial.</p> </li> </ul> Source code in <code>src/qtt/optimizers/quick.py</code> <pre><code>def tell(self, result: dict | list):\n    \"\"\"Tell the result of a trial to the optimizer.\n\n    Args:\n        result: dict | list[dict]\n            The result(s) for a trial.\n    \"\"\"\n    if isinstance(result, dict):\n        result = [result]\n    for res in result:\n        self._tell(res)\n</code></pre>"},{"location":"reference/optimizers/random/","title":"Random","text":""},{"location":"reference/optimizers/random/#qtt.optimizers.random.RandomOptimizer","title":"<code>RandomOptimizer</code>","text":"<p>               Bases: <code>Optimizer</code></p> <p>Random search optimizer.</p> <p>Parameters:</p> <ul> <li> <code>cs</code>               (<code>ConfigurationSpace</code>)           \u2013            <p>Configuration space object.</p> </li> <li> <code>max_fidelity</code>               (<code>int</code>)           \u2013            <p>Maximum fidelity level.</p> </li> <li> <code>n</code>               (<code>int</code>)           \u2013            <p>Number of configurations to sample.</p> </li> <li> <code>patience</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Determines if early stopping should be applied for a single configuration. If the score does not improve for <code>patience</code> steps, the configuration is stopped. Defaults to None.</p> </li> <li> <code>tol</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Tolerance for early stopping. Training stops if the score does not improve by at least <code>tol</code> for <code>patience</code> iterations (if set). Values must be in the range <code>[0.0, inf)</code>. Defaults to 0.0.</p> </li> <li> <code>score_thresh</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Score threshold for early stopping. Defaults to 0.0.</p> </li> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to save the optimizer. Defaults to None.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Random seed. Defaults to None.</p> </li> <li> <code>verbosity</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity level. Defaults to 2.</p> </li> </ul> Source code in <code>src/qtt/optimizers/random.py</code> <pre><code>class RandomOptimizer(Optimizer):\n    \"\"\"Random search optimizer.\n\n    Args:\n        cs (ConfigurationSpace): Configuration space object.\n        max_fidelity (int): Maximum fidelity level.\n        n (int): Number of configurations to sample.\n        patience (int, optional): Determines if early stopping should be applied for a\n            single configuration. If the score does not improve for `patience` steps,\n            the configuration is stopped. Defaults to None.\n        tol (float, optional): Tolerance for early stopping. Training stops if the score\n            does not improve by at least `tol` for `patience` iterations (if set). Values\n            must be in the range `[0.0, inf)`. Defaults to 0.0.\n        score_thresh (float, optional): Score threshold for early stopping. Defaults to 0.0.\n        path (str, optional): Path to save the optimizer. Defaults to None.\n        seed (int, optional): Random seed. Defaults to None.\n        verbosity (int, optional): Verbosity level. Defaults to 2.\n    \"\"\"\n    def __init__(\n        self,\n        cs: ConfigurationSpace,\n        max_fidelity: int,\n        n: int,\n        *,\n        patience: int | None = None,\n        tol: float = 0.0,\n        score_thresh: float = 0.0,\n        #\n        path: str | None = None,\n        seed: int | None = None,\n        verbosity: int = 2,\n    ):\n        super().__init__(path=path)\n        set_logger_verbosity(verbosity, logger)\n        self.verbosity = verbosity\n\n        if seed is not None:\n            fix_random_seeds(seed)\n        self.seed = seed\n\n        self.cs = cs\n        self.max_fidelity = max_fidelity\n        self.candidates = cs.sample_configuration(n)\n        self.N = n\n        self.patience = patience\n        self.tol = tol\n        self.scr_thr = score_thresh\n\n        self.reset()\n\n    def reset(self):\n        # trackers\n        self.iteration = 0\n        self.ask_count = 0\n        self.tell_count = 0\n        self.init_count = 0\n        self.eval_count = 0\n        self.evaled = set()\n        self.stoped = set()\n        self.failed = set()\n        self.history = []\n\n        self.fidelities: np.ndarray = np.zeros(self.N, dtype=int)\n        self.curves: np.ndarray = np.zeros((self.N, self.max_fidelity), dtype=float)\n        self.costs: np.ndarray = np.zeros(self.N, dtype=float)\n\n        if self.patience is not None:\n            self._score_history = np.zeros((self.N, self.patience), dtype=float)\n\n    def ask(self):\n        left = set(range(self.N)) - self.failed - self.stoped\n        index = random.choice(list(left))\n\n        fidelity = self.fidelities[index] + 1\n\n        return {\n            \"config_id\": index,\n            \"config\": self.candidates[index],\n            \"fidelity\": fidelity,\n        }\n\n    def tell(self, reports: dict | list):\n        if isinstance(reports, dict):\n            reports = [reports]\n        for report in reports:\n            self._tell(report)\n\n    def _tell(self, report: dict):\n        self.tell_count += 1\n\n        index = report[\"config_id\"]\n        fidelity = report[\"fidelity\"]\n        cost = report[\"cost\"]\n        score = report[\"score\"]\n        status = report[\"status\"]\n\n        if not status:\n            self.failed.add(index)\n            return\n\n        # update trackers\n        self.curves[index, fidelity - 1] = score\n        self.fidelities[index] = fidelity\n        self.costs[index] = cost\n        self.history.append(report)\n        self.evaled.add(index)\n        self.eval_count += 1\n\n        if score &gt;= 1.0 - self.scr_thr or fidelity == self.max_fidelity:\n            self.stoped.add(index)\n\n        if self.patience is not None:\n            if not np.any(self._score_history[index] &lt; (score - self.tol)):\n                self.stoped.add(index)\n            self._score_history[index][fidelity % self.patience] = score\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.random.RandomOptimizer.ante","title":"<code>ante()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some pre-processing steps before each ask.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def ante(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some pre-processing steps before each ask.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.random.RandomOptimizer.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). The model file is typically located in os.path.join(path, cls.model_file_name).</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the self.path value of the loaded model to be equal to path. It is highly recommended to keep this value as True unless accessing the original self.path value is important. If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Optimizer</code> )          \u2013            <pre><code>Loaded model object.\n</code></pre> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n    \"\"\"\n    Loads the model from disk to memory.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths (bool):\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose (bool):\n            Whether to log the location of the loaded file.\n\n    Returns:\n        model (Optimizer):\n            Loaded model object.\n    \"\"\"\n    file_path = os.path.join(path, cls.model_file_name)\n    with open(file_path, \"rb\") as f:\n        model = pickle.load(f)\n    if reset_paths:\n        model.path = path\n    if verbose:\n        logger.info(f\"Model loaded from: {file_path}\")\n    return model\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.random.RandomOptimizer.post","title":"<code>post()</code>","text":"<p>This method is intended for the use with a tuner. It allows to perform some post-processing steps after each tell.</p> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def post(self):\n    \"\"\"This method is intended for the use with a tuner.\n    It allows to perform some post-processing steps after each tell.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.random.RandomOptimizer.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"Reset the path of the model.\n\n    Args:\n        path (str):\n            Directory location to store all outputs.\n            If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower(), path_suffix=self.name)\n    self.path = path\n</code></pre>"},{"location":"reference/optimizers/random/#qtt.optimizers.random.RandomOptimizer.save","title":"<code>save(path=None, verbose=True)</code>","text":"<p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). If None, self.path is used. The final model file is typically saved to os.path.join(path, self.model_file_name).</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the saved file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>path</code> (              <code>str</code> )          \u2013            <pre><code>Path to the saved model, minus the file name.\nUse this value to load the model from disk via cls.load(path), cls being the class of the model object, such as model = RFModel.load(path)\n</code></pre> </li> </ul> Source code in <code>src/qtt/optimizers/optimizer.py</code> <pre><code>def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Saves the model to disk.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            If None, self.path is used.\n            The final model file is typically saved to os.path.join(path, self.model_file_name).\n        verbose (bool):\n            Whether to log the location of the saved file.\n\n    Returns:\n        path (str):\n            Path to the saved model, minus the file name.\n            Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as model = RFModel.load(path)\n    \"\"\"\n    if path is None:\n        path = self.path\n    os.makedirs(path, exist_ok=True)\n    file_path = os.path.join(path, self.model_file_name)\n    # tmp = {}\n    # for key, obj in vars(self).items():\n    #     if hasattr(obj, \"save\"):\n    #         obj_path = os.path.join(path, key)\n    #         obj.save(obj_path)\n    #         tmp[key] = obj\n    #         setattr(self, key, None)\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n    # for key, obj in tmp.items():\n    #     setattr(self, key, obj)\n    if verbose:\n        logger.info(f\"Model saved to: {file_path}\")\n    return path\n</code></pre>"},{"location":"reference/predictors/","title":"Overview","text":""},{"location":"reference/predictors/cost/","title":"CostPredictor","text":""},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor","title":"<code>CostPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> Source code in <code>src/qtt/predictors/cost.py</code> <pre><code>class CostPredictor(Predictor):\n    temp_file_name = \"temp_model.pt\"\n\n    def __init__(\n        self,\n        fit_params: dict = {},\n        # refit_params: dict = {},\n        path: str | None = None,\n        seed: int | None = None,\n        verbosity: int = 2,\n    ) -&gt; None:\n        super().__init__(path=path)\n\n        self.fit_params = self._validate_fit_params(fit_params, DEFAULT_FIT_PARAMS)\n        self.seed = seed\n        self.verbose = verbosity\n\n        set_logger_verbosity(verbosity, logger)\n\n    @staticmethod\n    def _validate_fit_params(fit_params, default_params):\n        if not isinstance(fit_params, dict):\n            raise ValueError(\"fit_params must be a dictionary\")\n        for key in fit_params:\n            if key not in default_params:\n                raise ValueError(f\"Unknown fit parameter: {key}\")\n        return {**default_params, **fit_params}\n\n    def _get_model(self):\n        params = {\n            \"in_dim\": [\n                len(self.types_of_features[\"continuous\"]),\n                len(self.types_of_features[\"categorical\"])\n                + len(self.types_of_features[\"bool\"]),\n            ],\n            \"enc_out_dim\": 16,\n            \"enc_nlayers\": 3,\n            \"enc_hidden_dim\": 128,\n        }\n        model = SimpleMLPRegressor(**params)\n        return model\n\n    def _validate_fit_data(self, X, y):\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"X must be a pandas.DataFrame instance\")\n\n        if not isinstance(y, np.ndarray):\n            raise ValueError(\"y must be a numpy.ndarray instance\")\n\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"X and y must have the same number of samples\")\n\n        if y.shape[1] != 1:\n            raise ValueError(\"y must have only one column\")\n\n        if len(set(X.columns)) &lt; len(X.columns):\n            raise ValueError(\n                \"Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})\"\n            )\n\n    def _validate_predict_data(self, pipeline):\n        if not isinstance(pipeline, pd.DataFrame):\n            raise ValueError(\"pipeline and curve must be pandas.DataFrame instances\")\n\n        if len(set(pipeline.columns)) &lt; len(pipeline.columns):\n            raise ValueError(\n                \"Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})\"\n            )\n\n    def _preprocess_fit_data(self, df: pd.DataFrame, array: np.ndarray):\n        \"\"\"\n        Process data for fitting the model.\n        \"\"\"\n        self._original_features = list(df.columns)\n\n        df, self.types_of_features, self.features_to_drop = get_types_of_features(df)\n        self._input_features = list(df.columns)\n        continous_features = self.types_of_features[\"continuous\"]\n        categorical_features = self.types_of_features[\"categorical\"]\n        bool_features = self.types_of_features[\"bool\"]\n        self.preprocessor = create_preprocessor(\n            continous_features,\n            categorical_features,\n            bool_features,\n        )\n        out = self.preprocessor.fit_transform(df)\n        self._feature_mapping = get_feature_mapping(self.preprocessor)\n        if out.shape[1] != sum(len(v) for v in self._feature_mapping.values()):\n            raise ValueError(\n                \"Error during one-hot encoding data processing for neural network. \"\n                \"Number of columns in df array does not match feature_mapping.\"\n            )\n\n        self.label_scaler = preprocessing.StandardScaler()  #MaxAbsScaler()\n        out_array = self.label_scaler.fit_transform(array)\n\n        return out, out_array\n\n    def _preprocess_predict_data(self, df: pd.DataFrame, fill_missing=True):\n        unexpected_columns = set(df.columns) - set(self._original_features)\n        if len(unexpected_columns) &gt; 0:\n            logger.warning(\n                \"Data contains columns that were not present during fitting: \"\n                f\"{unexpected_columns}\"\n            )\n\n        df = df.drop(columns=self.features_to_drop, errors=\"ignore\")\n\n        missing_columns = set(self._input_features) - set(df.columns)\n        if len(missing_columns) &gt; 0:\n            if fill_missing:\n                logger.warning(\n                    \"Data is missing columns that were present during fitting: \"\n                    f\"{missing_columns}. Trying to fill them with mean values / zeros.\"\n                )\n                for col in missing_columns:\n                    df[col] = None\n            else:\n                raise AssertionError(\n                    \"Data is missing columns that were present during fitting: \"\n                    f\"{missing_columns}. Please fill them with appropriate values.\"\n                )\n        X = self.preprocessor.transform(df)\n        X = np.array(X)\n        X = np.nan_to_num(X)\n        return X\n\n    def _fit_model(\n        self,\n        dataset,\n        learning_rate_init,\n        batch_size,\n        max_iter,\n        early_stop,\n        patience,\n        validation_fraction,\n        tol,\n    ):\n        if self.seed is not None:\n            random.seed(self.seed)\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.device = get_torch_device()\n        _dev = self.device\n        self.model.to(_dev)\n\n        optimizer = torch.optim.AdamW(self.model.parameters(), learning_rate_init)\n\n        patience_counter = 0\n        best_iter = 0\n        best_val_metric = np.inf\n\n        if patience is not None:\n            if early_stop:\n                if validation_fraction &lt; 0 or validation_fraction &gt; 1:\n                    raise AssertionError(\n                        \"validation_fraction must be between 0 and 1 when early_stop is True\"\n                    )\n                logger.info(\n                    f\"Early stopping on validation loss with patience {patience} \"\n                    f\"using {validation_fraction} of the data for validation\"\n                )\n                train_set, val_set = random_split(\n                    dataset=dataset,\n                    lengths=[1 - validation_fraction, validation_fraction],\n                )\n            else:\n                logger.info(f\"Early stopping on training loss with patience {patience}\")\n                train_set = dataset\n                val_set = None\n        else:\n            train_set = dataset\n            val_set = None\n\n        bs = min(batch_size, int(2 ** (3 + np.floor(np.log10(len(train_set))))))\n        train_loader = DataLoader(\n            train_set, batch_size=bs, shuffle=True, drop_last=True\n        )\n        val_loader = None\n        if val_set is not None:\n            bs = min(batch_size, int(2 ** (3 + np.floor(np.log10(len(val_set))))))\n            val_loader = DataLoader(val_set, batch_size=bs)\n\n        cache_dir = os.path.expanduser(\"~/.cache\")\n        cache_dir = os.path.join(cache_dir, \"qtt\", self.name)\n        os.makedirs(cache_dir, exist_ok=True)\n        temp_save_file_path = os.path.join(cache_dir, self.temp_file_name)\n        for it in range(1, max_iter + 1):\n            self.model.train()\n\n            train_loss = []\n            header = f\"TRAIN: ({it}/{max_iter})\"\n            metric_logger = MetricLogger(delimiter=\" \")\n            for batch in metric_logger.log_every(\n                train_loader, len(train_loader) // 10, header, logger\n            ):\n                # forward\n                batch = [item.to(_dev) for item in batch]\n                X, y = batch\n                loss = self.model.train_step(X, y)\n                train_loss.append(loss.item())\n\n                # update\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                metric_logger.update(loss=loss.item())\n            logger.info(f\"Averaged stats: {str(metric_logger)}\")\n            val_metric = np.mean(train_loss)\n\n            if val_loader is not None:\n                self.model.eval()\n\n                val_loss = []\n                with torch.no_grad():\n                    for batch in val_loader:\n                        batch = [item.to(_dev) for item in batch]\n                        X, y = batch\n                        pred = self.model.predict(X)\n                        loss = torch.nn.functional.l1_loss(pred, y)\n                        val_loss.append(loss.item())\n                val_metric = np.mean(val_loss)\n\n            if patience is not None:\n                if val_metric + tol &lt; best_val_metric:\n                    patience_counter = 0\n                    best_val_metric = val_metric\n                    best_iter = it\n                    torch.save(self.model.state_dict(), temp_save_file_path)\n                else:\n                    patience_counter += 1\n                logger.info(\n                    f\"VAL: {round(val_metric, 4)}  \"\n                    f\"ITER: {it}/{max_iter}  \"\n                    f\"BEST: {round(best_val_metric, 4)} ({best_iter})\"\n                )\n                if patience_counter &gt;= patience:\n                    logger.warning(\n                        \"Early stopping triggered! \"\n                        f\"No improvement in the last {patience} iterations. \"\n                        \"Stopping training...\"\n                    )\n                    break\n\n        if early_stop:\n            self.model.load_state_dict(torch.load(temp_save_file_path))\n\n    def _fit(\n        self,\n        X: pd.DataFrame,\n        y: np.ndarray,\n    ):\n        if self.is_fit:\n            raise AssertionError(\"Predictor is already fit! Create a new one.\")\n\n        self._validate_fit_data(X, y)\n        _X, _y = self._preprocess_fit_data(X, y)\n\n        train_dataset = SimpleTorchTabularDataset(_X, _y)\n\n        self.model = self._get_model()\n\n        self._fit_model(train_dataset, **self.fit_params)\n\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; np.ndarray:\n        if not self.is_fit:\n            raise AssertionError(\"Model is not fitted yet\")\n\n        self._validate_predict_data(X)\n\n        x = self._preprocess_predict_data(X)\n\n        device = self.device\n\n        self.model.eval()\n        self.model.to(device)\n        x_t = torch.tensor(x, dtype=torch.float32).to(device)\n\n        with torch.no_grad():\n            pred = self.model.predict(x_t)\n        out = pred.cpu().squeeze().numpy()\n        return out\n\n    def save(self, path: str | None = None, verbose=True) -&gt; str:\n        # Save on CPU to ensure the model can be loaded on a box without GPU\n        if self.model is not None:\n            self.model = self.model.to(torch.device(\"cpu\"))\n        path = super().save(path, verbose)\n        # Put the model back to the device after the save\n        if self.model is not None:\n            self.model.to(self.device)\n        return path\n\n    @classmethod\n    def load(cls, path: str, reset_paths=True, verbose=True):\n        \"\"\"\n        Loads the model from disk to memory.\n        The loaded model will be on the same device it was trained on (cuda/mps);\n        if the device is it's not available (trained on GPU, deployed on CPU),\n        then `cpu` will be used.\n\n        Parameters\n        ----------\n        path : str\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths : bool, default True\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose : bool, default True\n            Whether to log the location of the loaded file.\n\n        Returns\n        -------\n        model : cls\n            Loaded model object.\n        \"\"\"\n        model: CostPredictor = super().load(\n            path=path, reset_paths=reset_paths, verbose=verbose\n        )\n        return model\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.is_fit","title":"<code>is_fit: bool</code>  <code>property</code>","text":"<p>Returns True if the model has been fit.</p>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.fit","title":"<code>fit(X, y, verbosity=2, **kwargs)</code>","text":"<p>Fit model to predict values in y based on X.</p> <p>Models should not override the <code>fit</code> method, but instead override the <code>_fit</code> method which has the same arguments.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>DataFrame</code>)           \u2013            <p>The training data features.</p> </li> <li> <code>y</code>               (<code>ArrayLike</code>)           \u2013            <p>The training data ground truth labels.</p> </li> <li> <code>verbosity</code>               (<code>int), default = 2</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity levels range from 0 to 4 and control how much information is printed. Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings). verbosity 4: logs every training iteration, and logs the most detailed information. verbosity 3: logs training iterations periodically, and logs more detailed information. verbosity 2: logs only important information. verbosity 1: logs only warnings and exceptions. verbosity 0: logs only exceptions.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Any additional fit arguments a model supports.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: ArrayLike, verbosity: int = 2, **kwargs):\n    \"\"\"\n    Fit model to predict values in y based on X.\n\n    Models should not override the `fit` method, but instead override the `_fit` method which has the same arguments.\n\n    Args:\n        X (pd.DataFrame):\n            The training data features.\n        y (ArrayLike):\n            The training data ground truth labels.\n        verbosity (int), default = 2:\n            Verbosity levels range from 0 to 4 and control how much information is printed.\n            Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n            verbosity 4: logs every training iteration, and logs the most detailed information.\n            verbosity 3: logs training iterations periodically, and logs more detailed information.\n            verbosity 2: logs only important information.\n            verbosity 1: logs only warnings and exceptions.\n            verbosity 0: logs only exceptions.\n        **kwargs :\n            Any additional fit arguments a model supports.\n    \"\"\"\n    out = self._fit(X=X, y=y, verbosity=verbosity, **kwargs)\n    if out is None:\n        out = self\n    return out\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory. The loaded model will be on the same device it was trained on (cuda/mps); if the device is it's not available (trained on GPU, deployed on CPU), then <code>cpu</code> will be used.</p>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.load--parameters","title":"Parameters","text":"<p>path : str     Path to the saved model, minus the file name.     This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).     The model file is typically located in os.path.join(path, cls.model_file_name). reset_paths : bool, default True     Whether to reset the self.path value of the loaded model to be equal to path.     It is highly recommended to keep this value as True unless accessing the original self.path value is important.     If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time. verbose : bool, default True     Whether to log the location of the loaded file.</p>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.load--returns","title":"Returns","text":"<p>model : cls     Loaded model object.</p> Source code in <code>src/qtt/predictors/cost.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths=True, verbose=True):\n    \"\"\"\n    Loads the model from disk to memory.\n    The loaded model will be on the same device it was trained on (cuda/mps);\n    if the device is it's not available (trained on GPU, deployed on CPU),\n    then `cpu` will be used.\n\n    Parameters\n    ----------\n    path : str\n        Path to the saved model, minus the file name.\n        This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n        The model file is typically located in os.path.join(path, cls.model_file_name).\n    reset_paths : bool, default True\n        Whether to reset the self.path value of the loaded model to be equal to path.\n        It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n        If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n    verbose : bool, default True\n        Whether to log the location of the loaded file.\n\n    Returns\n    -------\n    model : cls\n        Loaded model object.\n    \"\"\"\n    model: CostPredictor = super().load(\n        path=path, reset_paths=reset_paths, verbose=verbose\n    )\n    return model\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.preprocess","title":"<code>preprocess(**kwargs)</code>","text":"<p>Preprocesses the input data into internal form ready for fitting or inference.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def preprocess(self, **kwargs):\n    \"\"\"\n    Preprocesses the input data into internal form ready for fitting or inference.\n    \"\"\"\n    return self._preprocess(**kwargs)\n</code></pre>"},{"location":"reference/predictors/cost/#qtt.predictors.cost.CostPredictor.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"\n    Reset the path of the model.\n\n    Args:\n        path (str, optional):\n            Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower())\n    self.path = path\n</code></pre>"},{"location":"reference/predictors/perf/","title":"PerfPredictor","text":""},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor","title":"<code>PerfPredictor</code>","text":"<p>               Bases: <code>Predictor</code></p> Source code in <code>src/qtt/predictors/perf.py</code> <pre><code>class PerfPredictor(Predictor):\n    temp_file_name: str = \"temp_model.pt\"\n    train_data_size: int = 4096\n    _fit_data = None\n\n    def __init__(\n        self,\n        fit_params: dict = {},\n        refit_params: dict = {},\n        path: str | None = None,\n        seed: int | None = None,\n        verbosity: int = 2,\n    ) -&gt; None:\n        super().__init__(path=path)\n        self.fit_params = self._validate_fit_params(fit_params, DEFAULT_FIT_PARAMS)\n        self.refit_params = self._validate_fit_params(\n            refit_params, DEFAULT_REFIT_PARAMS\n        )\n        self.seed = seed\n        self.verbosity = verbosity\n\n        set_logger_verbosity(verbosity, logger)\n\n    @staticmethod\n    def _validate_fit_params(fit_params, default_params):\n        \"\"\"\n        Validate hyperparameters for fitting the model.\n\n        Args:\n            fit_params (dict): Hyperparameters for fitting the model.\n            default_params (dict): Default hyperparameters for fitting the model.\n\n        Raises:\n            ValueError: If fit_params is not a dictionary or contains unknown hyperparameters.\n\n        Returns:\n            dict: Validated hyperparameters.\n        \"\"\"\n        if not isinstance(fit_params, dict):\n            raise ValueError(\"fit_params must be a dictionary\")\n        for key in fit_params:\n            if key not in default_params:\n                raise ValueError(f\"Unknown fit parameter: {key}\")\n        return {**default_params, **fit_params}\n\n    def _validate_fit_data(self, pipeline: pd.DataFrame, curve: np.ndarray):\n        \"\"\"\n        Validate data for fitting the model.\n\n        Args:\n            pipeline (pandas.DataFrame): Pipeline data.\n            curve (numpy.ndarray): Curve data.\n\n        Raises:\n            ValueError: If pipeline or curve is not a pandas.DataFrame or numpy.ndarray, or if\n                pipeline and curve have different number of samples, or if column names are not\n                unique.\n\n        Returns:\n            tuple: Validated pipeline and curve data.\n        \"\"\"\n        if not isinstance(pipeline, pd.DataFrame):\n            raise ValueError(\"pipeline must be a pandas.DataFrame instance\")\n\n        if not isinstance(curve, np.ndarray):\n            raise ValueError(\"curve must be a numpy.ndarray instance\")\n\n        if pipeline.shape[0] != curve.shape[0]:\n            raise ValueError(\"pipeline and curve must have the same number of samples\")\n\n        if len(set(pipeline.columns)) &lt; len(pipeline.columns):\n            raise ValueError(\n                \"Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})\"\n            )\n\n        self._curve_dim = curve.shape[1]\n\n    def _preprocess_fit_data(self, df: pd.DataFrame) -&gt; np.ndarray:\n        \"\"\"\n        Preprocess data for fitting the model.\n\n        Args:\n            df (pandas.DataFrame): Data to preprocess.\n\n        Returns:\n            numpy.ndarray: Preprocessed data.\n        \"\"\"\n        self.original_features = list(df.columns)\n\n        df, self.types_of_features, self.features_to_drop = get_types_of_features(df)\n        self.input_features = list(df.columns)\n\n        self.preprocessor = create_preprocessor(\n            self.types_of_features[\"continuous\"],\n            self.types_of_features[\"categorical\"],\n            self.types_of_features[\"bool\"],\n        )\n        out = self.preprocessor.fit_transform(df)\n        self.feature_mapping = get_feature_mapping(self.preprocessor)\n        if out.shape[1] != sum(len(v) for v in self.feature_mapping.values()):\n            raise ValueError(\n                \"Error during one-hot encoding data processing for neural network. \"\n                \"Number of columns in df array does not match feature_mapping.\"\n            )\n        return np.array(out)\n\n    def _validate_predict_data(self, pipeline, curve):\n        \"\"\"Validate data for prediction. Applies the same steps as _validate_fit_data\n\n        Args:\n            pipeline (pandas.DataFrame): Pipeline data.\n            curve (numpy.ndarray): Curve data.\n\n        Raises:\n            ValueError: If pipeline or curve is not a pandas.DataFrame or numpy.ndarray, or if\n                pipeline and curve have different number of samples, or if column names are not\n                unique.\n\n        Returns:\n            tuple: Validated pipeline and curve data.\n        \"\"\"\n        if not isinstance(pipeline, pd.DataFrame) or not isinstance(curve, np.ndarray):\n            raise ValueError(\"pipeline and curve must be pandas.DataFrame instances\")\n\n        if pipeline.shape[0] != curve.shape[0]:\n            raise ValueError(\"pipeline and curve must have the same number of samples\")\n\n        if len(set(pipeline.columns)) &lt; len(pipeline.columns):\n            raise ValueError(\n                \"Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={'current_name':'new_name'})\"\n            )\n\n        if curve.shape[1] != self._curve_dim:\n            raise ValueError(\n                \"curve must have the same number of features as the curve used for fitting\"\n                \" (expected: {self._curve_length}, got: {curve.shape[1]})\"\n            )\n\n    def _preprocess_predict_data(self, df: pd.DataFrame, fill_missing=True):\n        extra_features = list(set(df.columns) - set(self.original_features))\n        if extra_features:\n            logger.warning(\n                f\"Features {extra_features} were not present in training data and are dropped\"\n            )\n            df = df.drop(columns=extra_features, errors=\"ignore\")\n\n        df = df.drop(columns=self.features_to_drop, errors=\"ignore\")\n\n        missing_features = list(set(self.input_features) - set(df.columns))\n        if missing_features:\n            if fill_missing:\n                logger.warning(\n                    f\"Features {missing_features} missing in data. Missing values will be imputed.\"\n                )\n                for col in missing_features:\n                    df[col] = None\n            else:\n                raise AssertionError(f\"Features {missing_features} missing in data.\")\n\n        # process data\n        X = self.preprocessor.transform(df)\n        X = np.array(X)\n        X = np.nan_to_num(X)\n        return X\n\n    def _get_model(self):\n        params = {\n            \"in_dim\": [\n                len(self.types_of_features[\"continuous\"]),\n                len(self.types_of_features[\"categorical\"])\n                + len(self.types_of_features[\"bool\"]),\n            ],\n            \"in_curve_dim\": self._curve_dim,\n        }\n        return SurrogateModel(**params)\n\n    def _train_model(\n        self,\n        dataset,\n        learning_rate_init: float,\n        batch_size: int,\n        max_iter: int,\n        early_stop: bool,\n        patience: int | None,\n        validation_fraction: float,\n        tol: float,\n    ):\n        \"\"\"\n        Train the model on the given dataset.\n\n        Args:\n            dataset: dataset to train on\n            learning_rate_init: initial learning rate\n            batch_size: batch size to use\n            max_iter: maximum number of iterations to train for\n            early_stop: if True, stop training when validation loss stops improving\n            patience: number of iterations to wait before stopping training\n            validation_fraction: fraction of dataset to use for validation\n            tol: tolerance for determining when to stop training\n        \"\"\"\n        if self.seed is not None:\n            random.seed(self.seed)\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n\n        self.device = get_torch_device()\n        dev = self.device\n        self.model.to(dev)\n\n        optimizer = torch.optim.AdamW(self.model.parameters(), learning_rate_init)\n\n        patience_counter = 0\n        best_iter = 0\n        best_val_metric = np.inf\n\n        if patience is not None:\n            if early_stop:\n                if validation_fraction &lt;= 0 or validation_fraction &gt;= 1:\n                    raise AssertionError(\n                        \"validation_fraction must be between 0 and 1 when early_stop is True\"\n                    )\n                logger.info(\n                    f\"Early stopping on validation loss with patience {patience} \"\n                    f\"using {validation_fraction} of the data for validation\"\n                )\n                train_set, val_set = random_split(\n                    dataset=dataset,\n                    lengths=[1 - validation_fraction, validation_fraction],\n                )\n            else:\n                logger.info(f\"Early stopping on training loss with patience {patience}\")\n                train_set = dataset\n                val_set = None\n        else:\n            train_set = dataset\n            val_set = None\n\n        train_loader = DataLoader(\n            train_set,\n            batch_size=min(batch_size, len(train_set)),\n            shuffle=True,\n            drop_last=True,\n            num_workers=4,\n        )\n        val_loader = None\n        if val_set is not None:\n            val_loader = DataLoader(\n                val_set,\n                batch_size=min(batch_size, len(val_set)),\n                num_workers=4,\n            )\n\n        cache_dir = os.path.join(self.path, \".tmp\")\n        os.makedirs(cache_dir, exist_ok=True)\n        temp_save_file_path = os.path.join(cache_dir, self.temp_file_name)\n        for it in range(1, max_iter + 1):\n            self.model.train()\n\n            train_loss = []\n            header = f\"TRAIN: ({it}/{max_iter})\"\n            metric_logger = MetricLogger(delimiter=\" \")\n            for batch in metric_logger.log_every(\n                train_loader, max(len(train_loader) // 10, 1), header, logger\n            ):\n                # forward\n                batch = (b.to(dev) for b in batch)\n                X, curve, y = batch\n                loss = self.model.train_step(X, curve, y)\n                train_loss.append(loss.item())\n\n                # update\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                # log\n                metric_logger.update(loss=loss.item())\n                metric_logger.update(lengthscale=self.model.lengthscale)\n                metric_logger.update(noise=self.model.noise)  # type: ignore\n            logger.info(f\"({it}/{max_iter}) Averaged stats: {str(metric_logger)}\")\n            val_metric = np.mean(train_loss)\n\n            if val_loader is not None:\n                self.model.eval()\n\n                val_loss = []\n                with torch.no_grad():\n                    for batch in val_loader:\n                        batch = (b.to(dev) for b in batch)\n                        X, curve, y = batch\n                        pred = self.model.predict(X, curve)\n                        loss = torch.nn.functional.l1_loss(pred.mean, y)\n                        val_loss.append(loss.item())\n                val_metric = np.mean(val_loss)\n\n            if patience is not None:\n                if val_metric + tol &lt; best_val_metric:\n                    patience_counter = 0\n                    best_val_metric = val_metric\n                    best_iter = it\n                    torch.save(self.model.state_dict(), temp_save_file_path)\n                else:\n                    patience_counter += 1\n                logger.info(\n                    f\"VAL: {round(val_metric, 4)}  \"\n                    f\"ITER: {it}/{max_iter}  \"\n                    f\"BEST: {round(best_val_metric, 4)} ({best_iter})\"\n                )\n                if patience_counter &gt;= patience:\n                    logger.log(\n                        15,\n                        \"Stopping training...\"\n                        f\"No improvement in the last {patience} iterations. \"\n                    )\n                    break\n\n        if early_stop:\n            logger.info(\n                f\"Loading best model from iteration {best_iter} with val score {best_val_metric}\"\n            )\n            self.model.load_state_dict(torch.load(temp_save_file_path))\n\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir)\n\n        # after training the gp, set its training data\n        # TODO: check if this can be improved\n        self.model.eval()\n        size = min(self.train_data_size, len(dataset))\n        loader = DataLoader(dataset, batch_size=size, shuffle=True)\n        a, b, c = next(iter(loader))\n        a, b, c = a.to(dev), b.to(dev), c.to(dev)\n        self.model.set_train_data(a, b, c)\n\n    def _fit(\n        self,\n        X: pd.DataFrame,\n        y: np.ndarray,\n        **kwargs,\n    ):\n        if self.is_fit:\n            raise AssertionError(\"Predictor is already fit! Create a new one.\")\n\n        self._validate_fit_data(X, y)\n        x = self._preprocess_fit_data(X)\n        train_dataset = CurveRegressionDataset(x, y)\n\n        self.model = self._get_model()\n        self._train_model(train_dataset, **self.fit_params)\n\n        self._model_fit = copy.deepcopy(self.model)\n        self._fit_data = train_dataset\n\n        return self\n\n    def fit_extra(\n        self,\n        X: pd.DataFrame,\n        curve: np.ndarray,\n        fit_params: dict = {},\n    ):\n        if not self.is_fit:\n            raise AssertionError(\"Model is not fitted yet\")\n\n        self._validate_predict_data(X, curve)\n\n        x = self._preprocess_predict_data(X)\n\n        tune_dataset = CurveRegressionDataset(x, curve)\n\n        fit_params = self._validate_fit_params(fit_params, self.refit_params)\n        self._refit_model(tune_dataset, **fit_params)\n\n    def _refit_model(\n        self,\n        dataset,\n        learning_rate_init,\n        batch_size,\n        max_iter,\n        early_stop,\n        patience,\n        tol,\n    ):\n        learning_rate_init = 0.001\n        logger.info(\"Refitting model...\")\n        if self.seed is not None:\n            random.seed(self.seed)\n            np.random.seed(self.seed)\n            torch.manual_seed(self.seed)\n        cache_dir = os.path.join(self.path, \".tmp\")\n        os.makedirs(cache_dir, exist_ok=True)\n        temp_save_file_path = os.path.join(cache_dir, self.temp_file_name)\n\n        num_workers = 4\n        self.device = get_torch_device()\n        dev = self.device\n\n        self.model.to(dev)\n        self.model.eval()\n        torch.save(self.model.state_dict(), temp_save_file_path)\n\n        # initial validation loss\n        loader = DataLoader(\n            dataset,\n            batch_size=min(len(dataset), batch_size),\n            num_workers=num_workers,\n        )\n        val_metric = []\n        for batch in loader:\n            batch = (b.to(dev) for b in batch)\n            X, curve, y = batch\n            pred = self.model.predict(X, curve)\n            loss = torch.nn.functional.l1_loss(pred.mean, y)\n            val_metric.append(loss.item())\n        best_val_metric = np.mean(val_metric)\n        logger.info(f\"Initial validation loss: {best_val_metric}\")\n        patience_counter = 0\n        best_iter = 0\n\n        assert self._fit_data is not None\n        fitting_set = self._fit_data\n        logger.debug(f\"Number of samples in the tuning set: {len(dataset)}\")\n        if len(dataset) &lt; batch_size:\n            logger.warning(\n                f\"Tuning-set size is small ({len(dataset)}).\"\n                \"Using all samples for training + validation. \"\n                f\"Adding samples from training set to reach minimal sample size {batch_size}\"\n            )\n\n        if patience is not None:\n            if early_stop:\n                logger.info(\n                    f\"Early stopping on validation loss with patience {patience} \"\n                )\n            else:\n                logger.info(f\"Early stopping on training loss with patience {patience}\")\n\n        loader_bs = min(int(2 ** np.floor(np.log2(len(dataset) - 1))), batch_size)\n        train_loader = DataLoader(\n            dataset,\n            batch_size=loader_bs,\n            shuffle=True,\n            drop_last=True,\n            num_workers=num_workers,\n        )\n        val_loader = DataLoader(\n            dataset,\n            batch_size=min(batch_size, len(dataset)),\n            num_workers=num_workers,\n        )\n        extra_loader = None\n        if loader_bs &lt; self.train_data_size:\n            extra_loader = DataLoader(\n                fitting_set,\n                batch_size=batch_size - loader_bs,\n                shuffle=True,\n                num_workers=num_workers,\n            )\n\n        optimizer = torch.optim.AdamW(self.model.parameters(), learning_rate_init)\n        for it in range(1, max_iter + 1):\n            self.model.train()\n\n            train_loss = []\n            header = f\"TRAIN: ({it}/{max_iter})\"\n            metric_logger = MetricLogger(delimiter=\" \")\n            for batch in metric_logger.log_every(train_loader, 1, header, logger):\n                # forward\n                if extra_loader is not None:\n                    b1 = next(iter(extra_loader))\n                    batch = [torch.cat([b1, b2]) for b1, b2 in zip(batch, b1)]\n                batch = (b.to(dev) for b in batch)\n                X, curve, y = batch\n                loss = self.model.train_step(X, curve, y)\n                train_loss.append(loss.item())\n\n                # update\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                # log\n                metric_logger.update(loss=loss.item())\n                metric_logger.update(lengthscale=self.model.lengthscale)\n                metric_logger.update(noise=self.model.noise)  # type: ignore\n            logger.info(f\"[{it}/{max_iter}]Averaged stats: {str(metric_logger)}\")\n            val_metric = np.mean(train_loss)\n\n            if val_loader is not None:\n                self.model.eval()\n\n                l1 = DataLoader(\n                    dataset,\n                    batch_size=len(dataset),\n                    shuffle=True,\n                )\n                batch = next(iter(l1))\n                if len(dataset) &lt; self.train_data_size:\n                    l2 = DataLoader(\n                        fitting_set,\n                        batch_size=self.train_data_size - loader_bs,\n                        shuffle=True,\n                    )\n                    b2 = next(iter(l2))\n                    batch = [torch.cat([p, q]) for p, q in zip(batch, b2)]\n                batch = (b.to(dev) for b in batch)\n                a, b, c = batch\n                self.model.set_train_data(a, b, c)\n\n                val_loss = []\n                with torch.no_grad():\n                    for batch in val_loader:\n                        batch = (b.to(dev) for b in batch)\n                        X, curve, y = batch\n                        pred = self.model.predict(X, curve)\n                        loss = torch.nn.functional.l1_loss(pred.mean, y)\n                        val_loss.append(loss.item())\n                val_metric = np.mean(val_loss)\n\n            if patience is not None:\n                if val_metric + tol &lt; best_val_metric:\n                    patience_counter = 0\n                    best_val_metric = val_metric\n                    best_iter = it\n                    torch.save(self.model.state_dict(), temp_save_file_path)\n                else:\n                    patience_counter += 1\n                logger.info(\n                    f\"[{it}/{max_iter}]  \"\n                    f\"VAL: {round(val_metric, 4)}  \"\n                    f\"BEST: {round(best_val_metric, 4)} ({best_iter})\",\n                )\n                if patience_counter &gt;= patience:\n                    logger.log(\n                        15,\n                        \"Stopping training...\"\n                        f\"No improvement in the last {patience} iterations. \",\n                    )\n                    break\n\n        if patience:\n            logger.info(f\"Loading best model from iteration {best_iter}\")\n            self.model.load_state_dict(torch.load(temp_save_file_path))\n\n        # remove cache dir\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir)\n\n        # after training the model, reset GPs training data\n        self.model.eval()\n        if len(dataset) &lt; self.train_data_size:\n            l1 = DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n            l2 = DataLoader(\n                fitting_set,\n                batch_size=self.train_data_size - len(dataset),\n                shuffle=True,\n            )\n            b1 = next(iter(l1))\n            b2 = next(iter(l2))\n            batch = [torch.cat([a, b]) for a, b in zip(b1, b2)]\n            batch = (b.to(dev) for b in batch)\n        else:\n            loader = DataLoader(dataset, batch_size=self.train_data_size, shuffle=True)\n            batch = next(iter(loader))\n            batch = (b.to(dev) for b in batch)\n        a, b, c = batch\n        self.model.set_train_data(a, b, c)\n\n    def predict(self, X: pd.DataFrame, curve: np.ndarray, fill_missing=True):\n        if not self.is_fit:\n            raise AssertionError(\"Model is not fitted yet\")\n\n        self._validate_predict_data(X, curve)\n        x = self._preprocess_predict_data(X, fill_missing)\n        curve = np.nan_to_num(curve)\n\n        device = self.device\n        self.model.eval()\n        self.model.to(device)\n        x = torch.tensor(x, dtype=torch.float32, device=device)\n        c = torch.tensor(curve, dtype=torch.float32, device=device)\n        mean = np.array([])\n        std = np.array([])\n        with torch.no_grad():\n            bs = 4096  # TODO: make this a parameter\n            for i in range(0, x.shape[0], bs):\n                pred = self.model.predict(x[i : i + bs], c[i : i + bs])\n                mean = np.append(mean, pred.mean.cpu().numpy())\n                std = np.append(std, pred.stddev.cpu().numpy())\n        return mean, std\n\n    def save(self, path: str | None = None, verbose=True) -&gt; str:\n        # Save on CPU to ensure the model can be loaded on a box without GPU\n        if self.model is not None:\n            self.model = self.model.to(torch.device(\"cpu\"))\n        path = super().save(path, verbose)\n        # Put the model back to the device after the save\n        if self.model is not None:\n            self.model.to(self.device)\n        return path\n\n    @classmethod\n    def load(cls, path: str, reset_paths=True, verbose=True) -&gt; \"PerfPredictor\":\n        \"\"\"\n        Loads the model from disk to memory.\n\n        The loaded model will be on the same device it was trained on (e.g., cuda/mps).\n        If the device is unavailable (e.g., trained on GPU but deployed on CPU),\n        the model will be loaded on `cpu`.\n\n        Args:\n            path (str): Path to the saved model, excluding the file name.\n                This should typically be a directory path ending with a '/' character\n                (or appropriate path separator based on OS). The model file is usually\n                located at `os.path.join(path, cls.model_file_name)`.\n            reset_paths (bool, optional): Whether to reset the `self.path` value of the loaded\n                model to be equal to `path`. Defaults to True. Setting this to False may cause\n                inconsistencies between the actual valid path and `self.path`, potentially leading\n                to strange behavior and exceptions if the model needs to load other files later.\n            verbose (bool, optional): Whether to log the location of the loaded file. Defaults to True.\n\n        Returns:\n            cls: The loaded model object.\n        \"\"\"\n        model: PerfPredictor = super().load(\n            path=path, reset_paths=reset_paths, verbose=verbose\n        )\n\n        verbosity = model.verbosity\n        set_logger_verbosity(verbosity, logger)\n        return model\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.is_fit","title":"<code>is_fit: bool</code>  <code>property</code>","text":"<p>Returns True if the model has been fit.</p>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.fit","title":"<code>fit(X, y, verbosity=2, **kwargs)</code>","text":"<p>Fit model to predict values in y based on X.</p> <p>Models should not override the <code>fit</code> method, but instead override the <code>_fit</code> method which has the same arguments.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>DataFrame</code>)           \u2013            <p>The training data features.</p> </li> <li> <code>y</code>               (<code>ArrayLike</code>)           \u2013            <p>The training data ground truth labels.</p> </li> <li> <code>verbosity</code>               (<code>int), default = 2</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity levels range from 0 to 4 and control how much information is printed. Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings). verbosity 4: logs every training iteration, and logs the most detailed information. verbosity 3: logs training iterations periodically, and logs more detailed information. verbosity 2: logs only important information. verbosity 1: logs only warnings and exceptions. verbosity 0: logs only exceptions.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Any additional fit arguments a model supports.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: ArrayLike, verbosity: int = 2, **kwargs):\n    \"\"\"\n    Fit model to predict values in y based on X.\n\n    Models should not override the `fit` method, but instead override the `_fit` method which has the same arguments.\n\n    Args:\n        X (pd.DataFrame):\n            The training data features.\n        y (ArrayLike):\n            The training data ground truth labels.\n        verbosity (int), default = 2:\n            Verbosity levels range from 0 to 4 and control how much information is printed.\n            Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n            verbosity 4: logs every training iteration, and logs the most detailed information.\n            verbosity 3: logs training iterations periodically, and logs more detailed information.\n            verbosity 2: logs only important information.\n            verbosity 1: logs only warnings and exceptions.\n            verbosity 0: logs only exceptions.\n        **kwargs :\n            Any additional fit arguments a model supports.\n    \"\"\"\n    out = self._fit(X=X, y=y, verbosity=verbosity, **kwargs)\n    if out is None:\n        out = self\n    return out\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>The loaded model will be on the same device it was trained on (e.g., cuda/mps). If the device is unavailable (e.g., trained on GPU but deployed on CPU), the model will be loaded on <code>cpu</code>.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, excluding the file name. This should typically be a directory path ending with a '/' character (or appropriate path separator based on OS). The model file is usually located at <code>os.path.join(path, cls.model_file_name)</code>.</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the <code>self.path</code> value of the loaded model to be equal to <code>path</code>. Defaults to True. Setting this to False may cause inconsistencies between the actual valid path and <code>self.path</code>, potentially leading to strange behavior and exceptions if the model needs to load other files later.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cls</code> (              <code>PerfPredictor</code> )          \u2013            <p>The loaded model object.</p> </li> </ul> Source code in <code>src/qtt/predictors/perf.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths=True, verbose=True) -&gt; \"PerfPredictor\":\n    \"\"\"\n    Loads the model from disk to memory.\n\n    The loaded model will be on the same device it was trained on (e.g., cuda/mps).\n    If the device is unavailable (e.g., trained on GPU but deployed on CPU),\n    the model will be loaded on `cpu`.\n\n    Args:\n        path (str): Path to the saved model, excluding the file name.\n            This should typically be a directory path ending with a '/' character\n            (or appropriate path separator based on OS). The model file is usually\n            located at `os.path.join(path, cls.model_file_name)`.\n        reset_paths (bool, optional): Whether to reset the `self.path` value of the loaded\n            model to be equal to `path`. Defaults to True. Setting this to False may cause\n            inconsistencies between the actual valid path and `self.path`, potentially leading\n            to strange behavior and exceptions if the model needs to load other files later.\n        verbose (bool, optional): Whether to log the location of the loaded file. Defaults to True.\n\n    Returns:\n        cls: The loaded model object.\n    \"\"\"\n    model: PerfPredictor = super().load(\n        path=path, reset_paths=reset_paths, verbose=verbose\n    )\n\n    verbosity = model.verbosity\n    set_logger_verbosity(verbosity, logger)\n    return model\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.preprocess","title":"<code>preprocess(**kwargs)</code>","text":"<p>Preprocesses the input data into internal form ready for fitting or inference.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def preprocess(self, **kwargs):\n    \"\"\"\n    Preprocesses the input data into internal form ready for fitting or inference.\n    \"\"\"\n    return self._preprocess(**kwargs)\n</code></pre>"},{"location":"reference/predictors/perf/#qtt.predictors.perf.PerfPredictor.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"\n    Reset the path of the model.\n\n    Args:\n        path (str, optional):\n            Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower())\n    self.path = path\n</code></pre>"},{"location":"reference/predictors/predictor/","title":"Predictor","text":""},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor","title":"<code>Predictor</code>","text":"<p>Base class for all predictors.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. Defaults to None. If None, a new unique time-stamped directory is chosen.</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the subdirectory inside <code>path</code> where the model will be saved. The final model directory will be <code>os.path.join(path, name)</code>. If None, defaults to the model's class name: <code>self.__class__.__name__</code>.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>class Predictor:\n    \"\"\"\n    Base class for all predictors.\n\n    Args:\n        path (str, optional): Directory location to store all outputs. Defaults to None.\n            If None, a new unique time-stamped directory is chosen.\n        name (str, optional): Name of the subdirectory inside `path` where the model will be saved.\n            The final model directory will be `os.path.join(path, name)`.\n            If None, defaults to the model's class name: `self.__class__.__name__`.\n    \"\"\"\n\n    model_file_name = \"model.pkl\"\n\n    def __init__(\n        self,\n        name: str | None = None,\n        path: str | None = None,\n    ):\n        if name is None:\n            self.name = self.__class__.__name__\n            logger.info(\n                f\"No name was specified for model, defaulting to class name: {self.name}\",\n            )\n        else:\n            self.name = name\n\n        if path is None:\n            self.path: str = setup_outputdir(path=self.name.lower())\n            logger.info(\n                f\"No path was specified for predictor, defaulting to: {self.path}\",\n            )\n        else:\n            self.path = setup_outputdir(path)\n\n        self.model = None\n\n    def reset_path(self, path: str | None = None):\n        \"\"\"\n        Reset the path of the model.\n\n        Args:\n            path (str, optional):\n                Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.\n        \"\"\"\n        if path is None:\n            path = setup_outputdir(path=self.name.lower())\n        self.path = path\n\n    @property\n    def is_fit(self) -&gt; bool:\n        \"\"\"Returns True if the model has been fit.\"\"\"\n        return self.model is not None\n\n    def fit(self, X: pd.DataFrame, y: ArrayLike, verbosity: int = 2, **kwargs):\n        \"\"\"\n        Fit model to predict values in y based on X.\n\n        Models should not override the `fit` method, but instead override the `_fit` method which has the same arguments.\n\n        Args:\n            X (pd.DataFrame):\n                The training data features.\n            y (ArrayLike):\n                The training data ground truth labels.\n            verbosity (int), default = 2:\n                Verbosity levels range from 0 to 4 and control how much information is printed.\n                Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n                verbosity 4: logs every training iteration, and logs the most detailed information.\n                verbosity 3: logs training iterations periodically, and logs more detailed information.\n                verbosity 2: logs only important information.\n                verbosity 1: logs only warnings and exceptions.\n                verbosity 0: logs only exceptions.\n            **kwargs :\n                Any additional fit arguments a model supports.\n        \"\"\"\n        out = self._fit(X=X, y=y, verbosity=verbosity, **kwargs)\n        if out is None:\n            out = self\n        return out\n\n    def _fit(\n        self,\n        **kwargs,\n    ):\n        \"\"\"\n        Fit model to predict values in y based on X.\n\n        Models should override this method with their custom model fit logic.\n        X should not be assumed to be in a state ready for fitting to the inner model, and models may require special preprocessing in this method.\n        It is very important that `X = self.preprocess(X)` is called within `_fit`, or else `predict` and `predict_proba` may not work as intended.\n        It is also important that `_preprocess` is overwritten to properly clean the data.\n        Examples of logic that should be handled by a model include missing value handling, rescaling of features (if neural network), etc.\n        If implementing a new model, it is recommended to refer to existing model implementations and experiment using toy datasets.\n\n        Refer to `fit` method for documentation.\n        \"\"\"\n        raise NotImplementedError\n\n    def _preprocess(self, **kwargs):\n        \"\"\"\n        Data transformation logic should be added here.\n\n        Input data should not be trusted to be in a clean and ideal form, while the output should be in an ideal form for training/inference.\n        Examples of logic that should be added here include missing value handling, rescaling of features (if neural network), etc.\n        If implementing a new model, it is recommended to refer to existing model implementations and experiment using toy datasets.\n\n        In bagged ensembles, preprocessing code that lives in `_preprocess` will be executed on each child model once per inference call.\n        If preprocessing code could produce different output depending on the child model that processes the input data, then it must live here.\n        When in doubt, put preprocessing code here instead of in `_preprocess_nonadaptive`.\n        \"\"\"\n        raise NotImplementedError\n\n    def preprocess(self, **kwargs):\n        \"\"\"\n        Preprocesses the input data into internal form ready for fitting or inference.\n        \"\"\"\n        return self._preprocess(**kwargs)\n\n    @classmethod\n    def load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n        \"\"\"\n        Loads the model from disk to memory.\n\n        Args:\n            path (str):\n                Path to the saved model, minus the file name.\n                This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n                The model file is typically located in os.path.join(path, cls.model_file_name).\n            reset_paths (bool):\n                Whether to reset the self.path value of the loaded model to be equal to path.\n                It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n                If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n            verbose (bool):\n                Whether to log the location of the loaded file.\n\n        Returns:\n            model (Predictor):\n                Loaded model object.\n        \"\"\"\n        file_path = os.path.join(path, cls.model_file_name)\n        with open(file_path, \"rb\") as f:\n            model = pickle.load(f)\n        if reset_paths:\n            model.path = path\n        if verbose:\n            logger.info(f\"Model loaded from: {file_path}\")\n        return model\n\n    def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n        \"\"\"\n        Saves the model to disk.\n\n        Args:\n            path (str): Path to the saved model, minus the file name.\n                This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n                If None, self.path is used.\n                The final model file is typically saved to os.path.join(path, self.model_file_name).\n            verbose (bool): Whether to log the location of the saved file.\n\n        Returns:\n            path: Path to the saved model, minus the file name. Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as ```model = PerfPredictor.load(path)```\n        \"\"\"\n        if path is None:\n            path = self.path\n        path = setup_outputdir(path, create_dir=True, warn_if_exist=True)\n        file_path = os.path.join(path, self.model_file_name)\n        with open(file_path, \"wb\") as f:\n            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n        if verbose:\n            logger.info(f\"Model saved to: {file_path}\")\n        return path\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.is_fit","title":"<code>is_fit: bool</code>  <code>property</code>","text":"<p>Returns True if the model has been fit.</p>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.fit","title":"<code>fit(X, y, verbosity=2, **kwargs)</code>","text":"<p>Fit model to predict values in y based on X.</p> <p>Models should not override the <code>fit</code> method, but instead override the <code>_fit</code> method which has the same arguments.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>DataFrame</code>)           \u2013            <p>The training data features.</p> </li> <li> <code>y</code>               (<code>ArrayLike</code>)           \u2013            <p>The training data ground truth labels.</p> </li> <li> <code>verbosity</code>               (<code>int), default = 2</code>, default:                   <code>2</code> )           \u2013            <p>Verbosity levels range from 0 to 4 and control how much information is printed. Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings). verbosity 4: logs every training iteration, and logs the most detailed information. verbosity 3: logs training iterations periodically, and logs more detailed information. verbosity 2: logs only important information. verbosity 1: logs only warnings and exceptions. verbosity 0: logs only exceptions.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Any additional fit arguments a model supports.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def fit(self, X: pd.DataFrame, y: ArrayLike, verbosity: int = 2, **kwargs):\n    \"\"\"\n    Fit model to predict values in y based on X.\n\n    Models should not override the `fit` method, but instead override the `_fit` method which has the same arguments.\n\n    Args:\n        X (pd.DataFrame):\n            The training data features.\n        y (ArrayLike):\n            The training data ground truth labels.\n        verbosity (int), default = 2:\n            Verbosity levels range from 0 to 4 and control how much information is printed.\n            Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n            verbosity 4: logs every training iteration, and logs the most detailed information.\n            verbosity 3: logs training iterations periodically, and logs more detailed information.\n            verbosity 2: logs only important information.\n            verbosity 1: logs only warnings and exceptions.\n            verbosity 0: logs only exceptions.\n        **kwargs :\n            Any additional fit arguments a model supports.\n    \"\"\"\n    out = self._fit(X=X, y=y, verbosity=verbosity, **kwargs)\n    if out is None:\n        out = self\n    return out\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.load","title":"<code>load(path, reset_paths=True, verbose=True)</code>  <code>classmethod</code>","text":"<p>Loads the model from disk to memory.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). The model file is typically located in os.path.join(path, cls.model_file_name).</p> </li> <li> <code>reset_paths</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to reset the self.path value of the loaded model to be equal to path. It is highly recommended to keep this value as True unless accessing the original self.path value is important. If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the loaded file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>model</code> (              <code>Predictor</code> )          \u2013            <pre><code>Loaded model object.\n</code></pre> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>@classmethod\ndef load(cls, path: str, reset_paths: bool = True, verbose: bool = True):\n    \"\"\"\n    Loads the model from disk to memory.\n\n    Args:\n        path (str):\n            Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            The model file is typically located in os.path.join(path, cls.model_file_name).\n        reset_paths (bool):\n            Whether to reset the self.path value of the loaded model to be equal to path.\n            It is highly recommended to keep this value as True unless accessing the original self.path value is important.\n            If False, the actual valid path and self.path may differ, leading to strange behaviour and potential exceptions if the model needs to load any other files at a later time.\n        verbose (bool):\n            Whether to log the location of the loaded file.\n\n    Returns:\n        model (Predictor):\n            Loaded model object.\n    \"\"\"\n    file_path = os.path.join(path, cls.model_file_name)\n    with open(file_path, \"rb\") as f:\n        model = pickle.load(f)\n    if reset_paths:\n        model.path = path\n    if verbose:\n        logger.info(f\"Model loaded from: {file_path}\")\n    return model\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.preprocess","title":"<code>preprocess(**kwargs)</code>","text":"<p>Preprocesses the input data into internal form ready for fitting or inference.</p> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def preprocess(self, **kwargs):\n    \"\"\"\n    Preprocesses the input data into internal form ready for fitting or inference.\n    \"\"\"\n    return self._preprocess(**kwargs)\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.reset_path","title":"<code>reset_path(path=None)</code>","text":"<p>Reset the path of the model.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.</p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def reset_path(self, path: str | None = None):\n    \"\"\"\n    Reset the path of the model.\n\n    Args:\n        path (str, optional):\n            Directory location to store all outputs. If None, a new unique time-stamped directory is chosen.\n    \"\"\"\n    if path is None:\n        path = setup_outputdir(path=self.name.lower())\n    self.path = path\n</code></pre>"},{"location":"reference/predictors/predictor/#qtt.predictors.predictor.Predictor.save","title":"<code>save(path=None, verbose=True)</code>","text":"<p>Saves the model to disk.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Path to the saved model, minus the file name. This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS). If None, self.path is used. The final model file is typically saved to os.path.join(path, self.model_file_name).</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to log the location of the saved file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>path</code> (              <code>str</code> )          \u2013            <p>Path to the saved model, minus the file name. Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as <code>model = PerfPredictor.load(path)</code></p> </li> </ul> Source code in <code>src/qtt/predictors/predictor.py</code> <pre><code>def save(self, path: str | None = None, verbose: bool = True) -&gt; str:\n    \"\"\"\n    Saves the model to disk.\n\n    Args:\n        path (str): Path to the saved model, minus the file name.\n            This should generally be a directory path ending with a '/' character (or appropriate path separator value depending on OS).\n            If None, self.path is used.\n            The final model file is typically saved to os.path.join(path, self.model_file_name).\n        verbose (bool): Whether to log the location of the saved file.\n\n    Returns:\n        path: Path to the saved model, minus the file name. Use this value to load the model from disk via cls.load(path), cls being the class of the model object, such as ```model = PerfPredictor.load(path)```\n    \"\"\"\n    if path is None:\n        path = self.path\n    path = setup_outputdir(path, create_dir=True, warn_if_exist=True)\n    file_path = os.path.join(path, self.model_file_name)\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n    if verbose:\n        logger.info(f\"Model saved to: {file_path}\")\n    return path\n</code></pre>"},{"location":"reference/tuner/","title":"Index","text":""}]}